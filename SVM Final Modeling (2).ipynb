{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # Filter out warnings\n",
    "\n",
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spot Check Algorithm - helps with finding appropriate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined 513 models\n",
      ">0logistic: 0.669 (+/-0.054)\n",
      ">1logistic: 0.690 (+/-0.025)\n",
      ">2logistic: 0.690 (+/-0.025)\n",
      ">3logistic: 0.690 (+/-0.025)\n",
      ">0ridge-0.001: 0.644 (+/-0.057)\n",
      ">1ridge-0.001: 0.644 (+/-0.057)\n",
      ">2ridge-0.001: 0.644 (+/-0.057)\n",
      ">3ridge-0.001: 0.644 (+/-0.057)\n",
      ">0ridge-0.005: 0.644 (+/-0.057)\n",
      ">1ridge-0.005: 0.644 (+/-0.057)\n",
      ">2ridge-0.005: 0.644 (+/-0.057)\n",
      ">3ridge-0.005: 0.644 (+/-0.057)\n",
      ">0ridge-0.01: 0.644 (+/-0.057)\n",
      ">1ridge-0.01: 0.644 (+/-0.057)\n",
      ">2ridge-0.01: 0.644 (+/-0.057)\n",
      ">3ridge-0.01: 0.644 (+/-0.057)\n",
      ">0ridge-0.05: 0.644 (+/-0.057)\n",
      ">1ridge-0.05: 0.644 (+/-0.057)\n",
      ">2ridge-0.05: 0.644 (+/-0.057)\n",
      ">3ridge-0.05: 0.644 (+/-0.057)\n",
      ">0ridge-0.1: 0.644 (+/-0.057)\n",
      ">1ridge-0.1: 0.644 (+/-0.057)\n",
      ">2ridge-0.1: 0.644 (+/-0.057)\n",
      ">3ridge-0.1: 0.644 (+/-0.057)\n",
      ">0ridge-0.2: 0.644 (+/-0.057)\n",
      ">1ridge-0.2: 0.644 (+/-0.057)\n",
      ">2ridge-0.2: 0.644 (+/-0.057)\n",
      ">3ridge-0.2: 0.644 (+/-0.057)\n",
      ">0ridge-0.3: 0.644 (+/-0.057)\n",
      ">1ridge-0.3: 0.644 (+/-0.057)\n",
      ">2ridge-0.3: 0.644 (+/-0.057)\n",
      ">3ridge-0.3: 0.644 (+/-0.057)\n",
      ">0ridge-0.4: 0.644 (+/-0.057)\n",
      ">1ridge-0.4: 0.644 (+/-0.057)\n",
      ">2ridge-0.4: 0.644 (+/-0.057)\n",
      ">3ridge-0.4: 0.644 (+/-0.057)\n",
      ">0ridge-0.5: 0.644 (+/-0.057)\n",
      ">1ridge-0.5: 0.644 (+/-0.057)\n",
      ">2ridge-0.5: 0.644 (+/-0.057)\n",
      ">3ridge-0.5: 0.644 (+/-0.057)\n",
      ">0ridge-0.6: 0.644 (+/-0.057)\n",
      ">1ridge-0.6: 0.644 (+/-0.057)\n",
      ">2ridge-0.6: 0.644 (+/-0.057)\n",
      ">3ridge-0.6: 0.644 (+/-0.057)\n",
      ">0ridge-0.7: 0.644 (+/-0.057)\n",
      ">1ridge-0.7: 0.644 (+/-0.057)\n",
      ">2ridge-0.7: 0.644 (+/-0.057)\n",
      ">3ridge-0.7: 0.644 (+/-0.057)\n",
      ">0ridge-0.8: 0.669 (+/-0.054)\n",
      ">1ridge-0.8: 0.644 (+/-0.057)\n",
      ">2ridge-0.8: 0.665 (+/-0.045)\n",
      ">3ridge-0.8: 0.665 (+/-0.045)\n",
      ">0ridge-0.9: 0.669 (+/-0.054)\n",
      ">1ridge-0.9: 0.644 (+/-0.057)\n",
      ">2ridge-0.9: 0.690 (+/-0.025)\n",
      ">3ridge-0.9: 0.690 (+/-0.025)\n",
      ">0ridge-1.0: 0.669 (+/-0.054)\n",
      ">1ridge-1.0: 0.644 (+/-0.057)\n",
      ">2ridge-1.0: 0.690 (+/-0.025)\n",
      ">3ridge-1.0: 0.690 (+/-0.025)\n",
      ">0ridge-2: 0.669 (+/-0.054)\n",
      ">1ridge-2: 0.644 (+/-0.057)\n",
      ">2ridge-2: 0.690 (+/-0.025)\n",
      ">3ridge-2: 0.690 (+/-0.025)\n",
      ">0ridge-5: 0.669 (+/-0.054)\n",
      ">1ridge-5: 0.669 (+/-0.054)\n",
      ">2ridge-5: 0.690 (+/-0.025)\n",
      ">3ridge-5: 0.690 (+/-0.025)\n",
      ">0ridge-10: 0.669 (+/-0.054)\n",
      ">1ridge-10: 0.669 (+/-0.054)\n",
      ">2ridge-10: 0.690 (+/-0.025)\n",
      ">3ridge-10: 0.690 (+/-0.025)\n",
      ">0sgd: 0.690 (+/-0.025)\n",
      ">1sgd: 0.644 (+/-0.057)\n",
      ">2sgd: 0.565 (+/-0.212)\n",
      ">3sgd: 0.452 (+/-0.217)\n",
      ">0pa: 0.590 (+/-0.169)\n",
      ">1pa: 0.624 (+/-0.062)\n",
      ">2pa: 0.545 (+/-0.150)\n",
      ">3pa: 0.667 (+/-0.063)\n",
      ">0knn-1: 0.518 (+/-0.109)\n",
      ">1knn-1: 0.715 (+/-0.055)\n",
      ">2knn-1: 0.736 (+/-0.048)\n",
      ">3knn-1: 0.736 (+/-0.048)\n",
      ">0knn-2: 0.672 (+/-0.080)\n",
      ">1knn-2: 0.775 (+/-0.054)\n",
      ">2knn-2: 0.773 (+/-0.061)\n",
      ">3knn-2: 0.773 (+/-0.061)\n",
      ">0knn-3: 0.651 (+/-0.148)\n",
      ">1knn-3: 0.688 (+/-0.093)\n",
      ">2knn-3: 0.703 (+/-0.123)\n",
      ">3knn-3: 0.703 (+/-0.123)\n",
      ">0knn-4: 0.669 (+/-0.054)\n",
      ">1knn-4: 0.667 (+/-0.023)\n",
      ">2knn-4: 0.690 (+/-0.025)\n",
      ">3knn-4: 0.690 (+/-0.025)\n",
      ">0knn-5: 0.603 (+/-0.083)\n",
      ">1knn-5: 0.597 (+/-0.063)\n",
      ">2knn-5: 0.619 (+/-0.086)\n",
      ">3knn-5: 0.619 (+/-0.086)\n",
      ">0knn-6: 0.648 (+/-0.088)\n",
      ">1knn-6: 0.622 (+/-0.032)\n",
      ">2knn-6: 0.669 (+/-0.054)\n",
      ">3knn-6: 0.669 (+/-0.054)\n",
      ">0knn-7: 0.623 (+/-0.084)\n",
      ">1knn-7: 0.599 (+/-0.044)\n",
      ">2knn-7: 0.647 (+/-0.043)\n",
      ">3knn-7: 0.647 (+/-0.043)\n",
      ">0knn-8: 0.690 (+/-0.025)\n",
      ">1knn-8: 0.647 (+/-0.043)\n",
      ">2knn-8: 0.690 (+/-0.025)\n",
      ">3knn-8: 0.690 (+/-0.025)\n",
      ">0knn-9: 0.601 (+/-0.063)\n",
      ">1knn-9: 0.582 (+/-0.112)\n",
      ">2knn-9: 0.665 (+/-0.045)\n",
      ">3knn-9: 0.665 (+/-0.045)\n",
      ">0cart: 0.516 (+/-0.067)\n",
      ">1cart: 0.516 (+/-0.067)\n",
      ">2cart: 0.491 (+/-0.047)\n",
      ">3cart: 0.516 (+/-0.067)\n",
      ">0extra: 0.559 (+/-0.093)\n",
      ">1extra: 0.694 (+/-0.115)\n",
      ">2extra: 0.484 (+/-0.134)\n",
      ">3extra: 0.628 (+/-0.124)\n",
      ">0svmr0.1: 0.523 (+/-0.190)\n",
      ">1svmr0.1: 0.523 (+/-0.190)\n",
      ">2svmr0.1: 0.523 (+/-0.190)\n",
      ">3svmr0.1: 0.523 (+/-0.190)\n",
      ">0svml0.1: 0.487 (+/-0.176)\n",
      ">1svml0.1: 0.545 (+/-0.150)\n",
      ">2svml0.1: 0.473 (+/-0.162)\n",
      ">3svml0.1: 0.473 (+/-0.162)\n",
      ">0svmp0.11: 0.560 (+/-0.188)\n",
      ">1svmp0.11: 0.473 (+/-0.162)\n",
      ">2svmp0.11: 0.523 (+/-0.190)\n",
      ">3svmp0.11: 0.523 (+/-0.190)\n",
      ">0svmp0.12: 0.459 (+/-0.103)\n",
      ">1svmp0.12: 0.523 (+/-0.190)\n",
      ">2svmp0.12: 0.523 (+/-0.190)\n",
      ">3svmp0.12: 0.523 (+/-0.190)\n",
      ">0svmp0.13: 0.541 (+/-0.103)\n",
      ">1svmp0.13: 0.445 (+/-0.083)\n",
      ">2svmp0.13: 0.523 (+/-0.190)\n",
      ">3svmp0.13: 0.523 (+/-0.190)\n",
      ">0svmp0.14: 0.526 (+/-0.145)\n",
      ">1svmp0.14: 0.352 (+/-0.066)\n",
      ">2svmp0.14: 0.523 (+/-0.190)\n",
      ">3svmp0.14: 0.523 (+/-0.190)\n",
      ">0svmp0.15: error\n",
      ">1svmp0.15: 0.356 (+/-0.057)\n",
      ">2svmp0.15: 0.523 (+/-0.190)\n",
      ">3svmp0.15: 0.523 (+/-0.190)\n",
      ">0svmp0.16: error\n",
      ">1svmp0.16: 0.353 (+/-0.043)\n",
      ">2svmp0.16: 0.523 (+/-0.190)\n",
      ">3svmp0.16: 0.523 (+/-0.190)\n",
      ">0svmp0.17: error\n",
      ">1svmp0.17: 0.353 (+/-0.043)\n",
      ">2svmp0.17: 0.523 (+/-0.190)\n",
      ">3svmp0.17: 0.523 (+/-0.190)\n",
      ">0svmp0.18: error\n",
      ">1svmp0.18: 0.378 (+/-0.032)\n",
      ">2svmp0.18: 0.523 (+/-0.190)\n",
      ">3svmp0.18: 0.523 (+/-0.190)\n",
      ">0svmp0.19: error\n",
      ">1svmp0.19: 0.374 (+/-0.076)\n",
      ">2svmp0.19: 0.523 (+/-0.190)\n",
      ">3svmp0.19: 0.523 (+/-0.190)\n",
      ">0svmp0.110: error\n",
      ">1svmp0.110: 0.378 (+/-0.032)\n",
      ">2svmp0.110: 0.523 (+/-0.190)\n",
      ">3svmp0.110: 0.523 (+/-0.190)\n",
      ">0svmp0.111: error\n",
      ">1svmp0.111: 0.395 (+/-0.111)\n",
      ">2svmp0.111: 0.523 (+/-0.190)\n",
      ">3svmp0.111: 0.523 (+/-0.190)\n",
      ">0svmp0.112: error\n",
      ">1svmp0.112: 0.399 (+/-0.022)\n",
      ">2svmp0.112: 0.523 (+/-0.190)\n",
      ">3svmp0.112: 0.523 (+/-0.190)\n",
      ">0svmp0.113: error\n",
      ">1svmp0.113: 0.395 (+/-0.111)\n",
      ">2svmp0.113: 0.523 (+/-0.190)\n",
      ">3svmp0.113: 0.523 (+/-0.190)\n",
      ">0svmp0.114: 0.310 (+/-0.025)\n",
      ">1svmp0.114: 0.420 (+/-0.050)\n",
      ">2svmp0.114: 0.523 (+/-0.190)\n",
      ">3svmp0.114: 0.523 (+/-0.190)\n",
      ">0svmp0.115: 0.310 (+/-0.025)\n",
      ">1svmp0.115: 0.395 (+/-0.111)\n",
      ">2svmp0.115: 0.523 (+/-0.190)\n",
      ">3svmp0.115: 0.523 (+/-0.190)\n",
      ">0svmr0.2: 0.523 (+/-0.190)\n",
      ">1svmr0.2: 0.523 (+/-0.190)\n",
      ">2svmr0.2: 0.523 (+/-0.190)\n",
      ">3svmr0.2: 0.523 (+/-0.190)\n",
      ">0svml0.2: 0.433 (+/-0.199)\n",
      ">1svml0.2: 0.503 (+/-0.118)\n",
      ">2svml0.2: 0.423 (+/-0.176)\n",
      ">3svml0.2: 0.423 (+/-0.176)\n",
      ">0svmp0.21: 0.512 (+/-0.197)\n",
      ">1svmp0.21: 0.441 (+/-0.093)\n",
      ">2svmp0.21: 0.523 (+/-0.190)\n",
      ">3svmp0.21: 0.523 (+/-0.190)\n",
      ">0svmp0.22: 0.557 (+/-0.038)\n",
      ">1svmp0.22: 0.420 (+/-0.097)\n",
      ">2svmp0.22: 0.523 (+/-0.190)\n",
      ">3svmp0.22: 0.523 (+/-0.190)\n",
      ">0svmp0.23: error\n",
      ">1svmp0.23: 0.466 (+/-0.083)\n",
      ">2svmp0.23: 0.523 (+/-0.190)\n",
      ">3svmp0.23: 0.523 (+/-0.190)\n",
      ">0svmp0.24: 0.526 (+/-0.145)\n",
      ">1svmp0.24: 0.374 (+/-0.076)\n",
      ">2svmp0.24: 0.523 (+/-0.190)\n",
      ">3svmp0.24: 0.523 (+/-0.190)\n"
     ]
    }
   ],
   "source": [
    "# binary classification spot check script\n",
    "import warnings\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    " \n",
    " \n",
    " # create a dict of standard models to evaluate {name:object}\n",
    "def define_models(models=dict()):\n",
    "\t# linear models\n",
    "\tmodels['logistic'] = LogisticRegression()\n",
    "\talpha = [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2, 5, 10]\n",
    "\tfor a in alpha:\n",
    "\t\tmodels['ridge-'+str(a)] = RidgeClassifier(alpha=a)\n",
    "\tmodels['sgd'] = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "\tmodels['pa'] = PassiveAggressiveClassifier(max_iter=1000, tol=1e-3)\n",
    "\t# non-linear models\n",
    "\tn_neighbors = range(1, 10)\n",
    "\tfor k in n_neighbors:\n",
    "\t\tmodels['knn-'+str(k)] = KNeighborsClassifier(n_neighbors=k)\n",
    "\tmodels['cart'] = DecisionTreeClassifier()\n",
    "\tmodels['extra'] = ExtraTreeClassifier()\n",
    "\tc_values = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100]\n",
    "\tdegree = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "\tfor c in c_values:\n",
    "\t\tmodels['svmr'+str(c)] = SVC(C=c, class_weight = 'balanced')\n",
    "\t\tmodels['svml'+str(c)] = SVC(kernel='linear', C= c, class_weight = 'balanced')\n",
    "\t\tfor d in degree:\n",
    "\t\t\tmodels['svmp'+str(c) + str(d)] = SVC(kernel='poly', C=c, degree=d, class_weight = 'balanced')\n",
    "\tmodels['bayes'] = GaussianNB()\n",
    "\t# ensemble models\n",
    "\tn_trees = 100\n",
    "\tmodels['ada'] = AdaBoostClassifier(n_estimators=n_trees)\n",
    "\tmodels['bag'] = BaggingClassifier(n_estimators=n_trees)\n",
    "\tmodels['rf'] = RandomForestClassifier(n_estimators=n_trees)\n",
    "\tmodels['et'] = ExtraTreesClassifier(n_estimators=n_trees)\n",
    "\tmodels['gbm'] = GradientBoostingClassifier(n_estimators=n_trees)\n",
    "\tprint('Defined %d models' % len(models))\n",
    "\treturn models\n",
    " \n",
    "# no transforms pipeline\n",
    "def pipeline_none(model):\n",
    "\treturn model\n",
    " \n",
    "# standardize transform pipeline\n",
    "def pipeline_standardize(model):\n",
    "\tsteps = list()\n",
    "\t# standardization\n",
    "\tsteps.append(('standardize', StandardScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    " \n",
    "# normalize transform pipeline\n",
    "def pipeline_normalize(model):\n",
    "\tsteps = list()\n",
    "\t# normalization\n",
    "\tsteps.append(('normalize', MinMaxScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    " \n",
    "# standardize and normalize pipeline\n",
    "def pipeline_std_norm(model):\n",
    "\tsteps = list()\n",
    "\t# standardization\n",
    "\tsteps.append(('standardize', StandardScaler()))\n",
    "\t# normalization\n",
    "\tsteps.append(('normalize', MinMaxScaler()))\n",
    "\t# the model\n",
    "\tsteps.append(('model', model))\n",
    "\t# create pipeline\n",
    "\tpipeline = Pipeline(steps=steps)\n",
    "\treturn pipeline\n",
    " \n",
    "# evaluate a single model\n",
    "def evaluate_model(X_train, Y_train, model, folds, metric, pipe_func):\n",
    "\t# create the pipeline\n",
    "\tpipeline = pipe_func(model)\n",
    "\t# evaluate model\n",
    "\tscores = cross_val_score(pipeline, X_train, Y_train, scoring=metric, cv=folds, n_jobs=-1)\n",
    "\treturn scores\n",
    " \n",
    "# evaluate a model and try to trap errors and and hide warnings\n",
    "def robust_evaluate_model(X_train, Y_train, model, folds, metric, pipe_func):\n",
    "\tscores = None\n",
    "\ttry:\n",
    "\t\twith warnings.catch_warnings():\n",
    "\t\t\twarnings.filterwarnings(\"ignore\")\n",
    "\t\t\tscores = evaluate_model(X_train, Y_train, model, folds, metric, pipe_func)\n",
    "\texcept:\n",
    "\t\tscores = None\n",
    "\treturn scores\n",
    " \n",
    "# evaluate a dict of models {name:object}, returns {name:score}\n",
    "def evaluate_models(X_train, Y_train, models, pipe_funcs, folds=4, metric='accuracy'):\n",
    "\tresults = dict()\n",
    "\tfor name, model in models.items():\n",
    "\t\t# evaluate model under each preparation function\n",
    "\t\tfor i in range(len(pipe_funcs)):\n",
    "\t\t\t# evaluate the model\n",
    "\t\t\tscores = robust_evaluate_model(X_train, Y_train, model, folds, metric, pipe_funcs[i])\n",
    "\t\t\t# update name\n",
    "\t\t\trun_name = str(i) + name\n",
    "\t\t\t# show process\n",
    "\t\t\tif scores is not None:\n",
    "\t\t\t\t# store a result\n",
    "\t\t\t\tresults[run_name] = scores\n",
    "\t\t\t\tmean_score, std_score = mean(scores), std(scores)\n",
    "\t\t\t\tprint('>%s: %.3f (+/-%.3f)' % (run_name, mean_score, std_score))\n",
    "\t\t\telse:\n",
    "\t\t\t\tprint('>%s: error' % run_name)\n",
    "\treturn results\n",
    " \n",
    "# print and plot the top n results\n",
    "def summarize_results(results, maximize=True, top_n=20):\n",
    "\t# check for no results\n",
    "\tif len(results) == 0:\n",
    "\t\tprint('no results')\n",
    "\t\treturn\n",
    "\t# determine how many results to summarize\n",
    "\tn = min(top_n, len(results))\n",
    "\t# create a list of (name, mean(scores)) tuples\n",
    "\tmean_scores = [(k,mean(v)) for k,v in results.items()]\n",
    "\t# sort tuples by mean score\n",
    "\tmean_scores = sorted(mean_scores, key=lambda x: x[1])\n",
    "\t# reverse for descending order (e.g. for accuracy)\n",
    "\tif maximize:\n",
    "\t\tmean_scores = list(reversed(mean_scores))\n",
    "\t# retrieve the top n for summarization\n",
    "\tnames = [x[0] for x in mean_scores[:n]]\n",
    "\tscores = [results[x[0]] for x in mean_scores[:n]]\n",
    "\t# print the top n\n",
    "\tprint()\n",
    "\tfor i in range(n):\n",
    "\t\tname = names[i]\n",
    "\t\tmean_score, std_score = mean(results[name]), std(results[name])\n",
    "\t\tprint('Rank=%d, Name=%s, Score=%.3f (+/- %.3f)' % (i+1, name, mean_score, std_score))\n",
    "\t# boxplot for the top n\n",
    "\tpyplot.boxplot(scores, labels=names)\n",
    "\t_, labels = pyplot.xticks()\n",
    "\tpyplot.setp(labels, rotation=90)\n",
    "\tpyplot.savefig('spotcheck.png')\n",
    " \n",
    "\n",
    "# get model list\n",
    "models = define_models()\n",
    "# define transform pipelines\n",
    "pipelines = [pipeline_none, pipeline_standardize, pipeline_normalize, pipeline_std_norm]\n",
    "# evaluate models\n",
    "results = evaluate_models(X_train, Y_train, models, pipelines)\n",
    "# summarize results\n",
    "summarize_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import files below:\n",
    "\n",
    "1. data1 = stepcount + demographic data in discrete categories\n",
    "2. d1 = demographic data in discrete categories\n",
    "3. data2 = stepcount + demographic data as continuous variables\n",
    "4. d2 = demographic data as a continuous variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = 'no_band_steps.xlsx'\n",
    "data1 = pd.read_excel(file1)\n",
    "data1 = data1.set_index('studyID')\n",
    "\n",
    "d1 = data1.drop(columns = 'Steps')\n",
    "\n",
    "file2 = 'band_steps.xlsx'\n",
    "data2 = pd.read_excel(file2)\n",
    "data2 = data2.set_index('studyID')\n",
    "\n",
    "d2 = data2.drop(columns = 'Steps_band')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demographic: No Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.8666666666666667\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6666666666666666\n",
      "PR AUC SCORE: 0.625\n",
      "Random State:  122\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.4166666666666667\n",
      "ROC AUC SCORE: 0.375\n",
      "PR AUC SCORE: 0.3\n",
      "Random State:  200\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.45714285714285713\n",
      "PR AUC SCORE: 0.4\n",
      "Random State:  300\n",
      "Training Accuracy: 0.7333333333333333\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6285714285714286\n",
      "PR AUC SCORE: 0.5166666666666666\n",
      "Random State:  368\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.4\n",
      "PR AUC SCORE: 0.16666666666666666\n",
      "Random State:  400\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.25\n",
      "Random State:  500\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5833333333333334\n",
      "PR AUC SCORE: 0.5555555555555556\n",
      "Random State:  600\n",
      "Training Accuracy: 0.7111111111111111\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.6875\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  700\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.4375\n",
      "PR AUC SCORE: 0.3125\n",
      "Random State:  22\n",
      "Training Accuracy: 0.7333333333333333\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7\n",
      "PR AUC SCORE: 0.65\n",
      "Mean Training Accuracy: 0.7933333333333333\n",
      "Mean Test Accuracy: 0.6083333333333334\n",
      "Mean ROC AUC score: 0.5435714285714286\n",
      "Mean PR AUC score: 0.42763888888888896\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "d1_train_acc = []\n",
    "d1_test_acc = []\n",
    "d1_roc = []\n",
    "d1_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(d1, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale', C= 20)\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    \n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "    \n",
    "    d1_train_acc.append(svm_train_acc)\n",
    "    d1_test_acc.append(svm_test_acc)\n",
    "    d1_roc.append(roc)\n",
    "    d1_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "print('Mean Training Accuracy:', mean(d1_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(d1_test_acc)) \n",
    "print('Mean ROC AUC score:', mean(d1_roc))\n",
    "print('Mean PR AUC score:', mean(d1_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demographic: Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6666666666666666\n",
      "PR AUC SCORE: 0.6666666666666667\n",
      "Random State:  122\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.625\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  200\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.4166666666666667\n",
      "Random State:  300\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6285714285714286\n",
      "PR AUC SCORE: 0.5166666666666666\n",
      "Random State:  368\n",
      "Training Accuracy: 0.7555555555555555\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.16666666666666666\n",
      "Random State:  400\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.4444444444444444\n",
      "PR AUC SCORE: 0.25\n",
      "Random State:  500\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  600\n",
      "Training Accuracy: 0.7555555555555555\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.8125\n",
      "PR AUC SCORE: 0.6458333333333334\n",
      "Random State:  700\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.4375\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  22\n",
      "Training Accuracy: 0.7555555555555555\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7\n",
      "PR AUC SCORE: 0.65\n",
      "Mean Training Accuracy: 0.7977777777777778\n",
      "Mean Test Accuracy: 0.6833333333333333\n",
      "Mean ROC AUC score: 0.581468253968254\n",
      "Mean PR AUC score: 0.4645833333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "d2_train_acc = []\n",
    "d2_test_acc = []\n",
    "d2_roc = []\n",
    "d2_pr = []\n",
    "\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(d2, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)  \n",
    "\n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "    \n",
    "    d2_train_acc.append(svm_train_acc)\n",
    "    d2_test_acc.append(svm_test_acc)\n",
    "    d2_roc.append(roc)\n",
    "    d2_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(d2_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(d2_test_acc)) \n",
    "print('Mean ROC AUC score:', mean(d2_roc))\n",
    "print('Mean PR AUC score:', mean(d2_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps + Demographic: No Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.6666666666666666\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5833333333333334\n",
      "PR AUC SCORE: 0.55\n",
      "Random State:  122\n",
      "Training Accuracy: 0.6444444444444445\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.5625\n",
      "PR AUC SCORE: 0.375\n",
      "Random State:  200\n",
      "Training Accuracy: 0.6222222222222222\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.45714285714285713\n",
      "PR AUC SCORE: 0.4\n",
      "Random State:  300\n",
      "Training Accuracy: 0.7333333333333333\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.45714285714285713\n",
      "PR AUC SCORE: 0.4\n",
      "Random State:  368\n",
      "Training Accuracy: 0.6888888888888889\n",
      "Test Accuracy: 0.4166666666666667\n",
      "ROC AUC SCORE: 0.25\n",
      "PR AUC SCORE: 0.16666666666666666\n",
      "Random State:  400\n",
      "Training Accuracy: 0.5555555555555556\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.7222222222222222\n",
      "PR AUC SCORE: 0.375\n",
      "Random State:  500\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5833333333333334\n",
      "PR AUC SCORE: 0.55\n",
      "Random State:  600\n",
      "Training Accuracy: 0.7111111111111111\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.625\n",
      "PR AUC SCORE: 0.41666666666666663\n",
      "Random State:  700\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.75\n",
      "PR AUC SCORE: 0.6666666666666666\n",
      "Random State:  22\n",
      "Training Accuracy: 0.5777777777777777\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5857142857142856\n",
      "PR AUC SCORE: 0.4666666666666667\n",
      "Mean Training Accuracy: 0.6755555555555556\n",
      "Mean Test Accuracy: 0.5916666666666666\n",
      "Mean ROC AUC score: 0.5576388888888889\n",
      "Mean PR AUC score: 0.43666666666666665\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "data1_train_acc = []\n",
    "data1_test_acc = []\n",
    "data1_roc = []\n",
    "data1_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(data1, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)  \n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "\n",
    "    \n",
    "    data1_train_acc.append(svm_train_acc)\n",
    "    data1_test_acc.append(svm_test_acc)\n",
    "    data1_roc.append(roc)\n",
    "    data1_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(data1_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(data1_test_acc))\n",
    "print('Mean ROC AUC score:', mean(data1_roc))\n",
    "print('Mean PR AUC score:', mean(data1_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps + Demographic: Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.75\n",
      "PR AUC SCORE: 0.75\n",
      "Random State:  122\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.625\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  200\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5285714285714286\n",
      "PR AUC SCORE: 0.43333333333333335\n",
      "Random State:  300\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7\n",
      "PR AUC SCORE: 0.65\n",
      "Random State:  368\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.3\n",
      "PR AUC SCORE: 0.16666666666666666\n",
      "Random State:  400\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.6666666666666666\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  500\n",
      "Training Accuracy: 0.8666666666666667\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5833333333333334\n",
      "PR AUC SCORE: 0.55\n",
      "Random State:  600\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.75\n",
      "PR AUC SCORE: 0.5333333333333333\n",
      "Random State:  700\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  22\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.9166666666666666\n",
      "ROC AUC SCORE: 0.9\n",
      "PR AUC SCORE: 0.8833333333333333\n",
      "Mean Training Accuracy: 0.8155555555555555\n",
      "Mean Test Accuracy: 0.7\n",
      "Mean ROC AUC score: 0.6303571428571428\n",
      "Mean PR AUC score: 0.53\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "data2_train_acc = []\n",
    "data2_test_acc = []\n",
    "data2_roc = []\n",
    "data2_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(data2, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "\n",
    "    \n",
    "    data2_train_acc.append(svm_train_acc)\n",
    "    data2_test_acc.append(svm_test_acc)\n",
    "    data2_roc.append(roc)\n",
    "    data2_pr.append(pr)\n",
    "\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(data2_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(data2_test_acc)) \n",
    "print('Mean ROC AUC score:', mean(data2_roc))\n",
    "print('Mean PR AUC score:', mean(data2_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important files below:\n",
    "\n",
    "1. no_band_final = KEGG + Demographic + stepcount data as continuous variables\n",
    "2. norm_no_band_final = KEGG + Demographic + stepcount data as continuous variables\n",
    "3. band_final = KEGG + Demographic + stepcount data as discrete variables\n",
    "4. norm_band_final = KEGG + Demographic + stepcount data as discrete variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_band_final = pd.read_excel('final_no_band.xlsx')\n",
    "no_band_final = no_band_final.set_index('studyID')\n",
    "\n",
    "norm_no_band_final = pd.read_excel('norm_final_no_band.xlsx')\n",
    "norm_no_band_final = norm_no_band_final.set_index('studyID')\n",
    "\n",
    "band_final = pd.read_excel('final_band.xlsx')\n",
    "band_final = band_final.set_index('studyID')\n",
    "\n",
    "norm_band_final = pd.read_excel('norm_final_band.xlsx')\n",
    "norm_band_final = norm_band_final.set_index('studyID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEGG + Demographic: No Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.75\n",
      "PR AUC SCORE: 0.7\n",
      "Random State:  122\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.9166666666666666\n",
      "ROC AUC SCORE: 0.875\n",
      "PR AUC SCORE: 0.8333333333333334\n",
      "Random State:  200\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7285714285714286\n",
      "PR AUC SCORE: 0.6166666666666667\n",
      "Random State:  300\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7285714285714286\n",
      "PR AUC SCORE: 0.6166666666666667\n",
      "Random State:  368\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.4166666666666667\n",
      "ROC AUC SCORE: 0.65\n",
      "PR AUC SCORE: 0.2222222222222222\n",
      "Random State:  400\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7571428571428572\n",
      "PR AUC SCORE: 0.6166666666666667\n",
      "Random State:  500\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.75\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  600\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.6111111111111112\n",
      "PR AUC SCORE: 0.3055555555555556\n",
      "Random State:  700\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.9166666666666666\n",
      "ROC AUC SCORE: 0.9444444444444444\n",
      "PR AUC SCORE: 0.75\n",
      "Random State:  22\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7222222222222222\n",
      "PR AUC SCORE: 0.41666666666666663\n",
      "Mean Training Accuracy: 1.0\n",
      "Mean Test Accuracy: 0.7250000000000001\n",
      "Mean ROC AUC score: 0.7517063492063493\n",
      "Mean PR AUC score: 0.5577777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "no_band_train_acc = []\n",
    "no_band_test_acc = []\n",
    "no_band_roc = []\n",
    "no_band_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(no_band_final, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']\n",
    "        \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train) \n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "    \n",
    "    no_band_train_acc.append(svm_train_acc)\n",
    "    no_band_test_acc.append(svm_test_acc)\n",
    "    no_band_roc.append(roc)\n",
    "    no_band_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(no_band_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(no_band_test_acc)) \n",
    "print('Mean ROC AUC score:', mean(no_band_roc))\n",
    "print('Mean PR AUC score:', mean(no_band_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEGG + Demographic: No Band,\n",
    "Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.75\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  122\n",
      "Training Accuracy: 0.7727272727272727\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.625\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  200\n",
      "Training Accuracy: 0.8409090909090909\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.45714285714285713\n",
      "PR AUC SCORE: 0.4\n",
      "Random State:  300\n",
      "Training Accuracy: 0.7045454545454546\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.4166666666666667\n",
      "Random State:  368\n",
      "Training Accuracy: 0.7727272727272727\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.7000000000000001\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  400\n",
      "Training Accuracy: 0.75\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.4285714285714286\n",
      "PR AUC SCORE: 0.4166666666666667\n",
      "Random State:  500\n",
      "Training Accuracy: 0.8181818181818182\n",
      "Test Accuracy: 0.4166666666666667\n",
      "ROC AUC SCORE: 0.375\n",
      "PR AUC SCORE: 0.3\n",
      "Random State:  600\n",
      "Training Accuracy: 0.7954545454545454\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.5555555555555556\n",
      "PR AUC SCORE: 0.2777777777777778\n",
      "Random State:  700\n",
      "Training Accuracy: 0.7727272727272727\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.25\n",
      "Random State:  22\n",
      "Training Accuracy: 0.6363636363636364\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.25\n",
      "Mean Training Accuracy: 0.7613636363636365\n",
      "Mean Test Accuracy: 0.625\n",
      "Mean ROC AUC score: 0.5141269841269842\n",
      "Mean PR AUC score: 0.36444444444444446\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "norm_no_band_train_acc = []\n",
    "norm_no_band_test_acc = []\n",
    "norm_no_band_roc = []\n",
    "norm_no_band_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(norm_no_band_final, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)  \n",
    "\n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "    \n",
    "    norm_no_band_train_acc.append(svm_train_acc)\n",
    "    norm_no_band_test_acc.append(svm_test_acc)\n",
    "    norm_no_band_roc.append(roc)\n",
    "    norm_no_band_pr.append(pr)\n",
    "\n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(norm_no_band_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(norm_no_band_test_acc)) \n",
    "print('Mean ROC AUC score:', mean(norm_no_band_roc))\n",
    "print('Mean PR AUC score:', mean(norm_no_band_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEGG + Demographic: Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6666666666666666\n",
      "PR AUC SCORE: 0.625\n",
      "Random State:  122\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6875\n",
      "PR AUC SCORE: 0.4583333333333333\n",
      "Random State:  200\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5571428571428572\n",
      "PR AUC SCORE: 0.45\n",
      "Random State:  300\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6571428571428573\n",
      "PR AUC SCORE: 0.5266666666666666\n",
      "Random State:  368\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5499999999999999\n",
      "PR AUC SCORE: 0.18333333333333335\n",
      "Random State:  400\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5571428571428572\n",
      "PR AUC SCORE: 0.45\n",
      "Random State:  500\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6875\n",
      "PR AUC SCORE: 0.4583333333333333\n",
      "Random State:  600\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.25\n",
      "Random State:  700\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.25\n",
      "Random State:  22\n",
      "Training Accuracy: 1.0\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.4444444444444444\n",
      "PR AUC SCORE: 0.25\n",
      "Mean Training Accuracy: 1.0\n",
      "Mean Test Accuracy: 0.625\n",
      "Mean ROC AUC score: 0.5807539682539683\n",
      "Mean PR AUC score: 0.39016666666666666\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "band_train_acc = []\n",
    "band_test_acc = []\n",
    "band_roc = []\n",
    "band_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(band_final, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)  \n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "    \n",
    "    band_train_acc.append(svm_train_acc)\n",
    "    band_test_acc.append(svm_test_acc)\n",
    "    band_roc.append(roc)\n",
    "    band_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(band_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(band_test_acc)) \n",
    "print('Mean ROC AUC score:', mean(band_roc))\n",
    "print('Mean PR AUC score:', mean(band_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KEGG + Demographic: Band, \n",
    "Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.75\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  122\n",
      "Training Accuracy: 0.7954545454545454\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  200\n",
      "Training Accuracy: 0.8181818181818182\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.45714285714285713\n",
      "PR AUC SCORE: 0.4\n",
      "Random State:  300\n",
      "Training Accuracy: 0.7045454545454546\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.4166666666666667\n",
      "Random State:  368\n",
      "Training Accuracy: 0.7272727272727273\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.7000000000000001\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  400\n",
      "Training Accuracy: 0.8181818181818182\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.4285714285714286\n",
      "PR AUC SCORE: 0.4166666666666667\n",
      "Random State:  500\n",
      "Training Accuracy: 0.8181818181818182\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.4375\n",
      "PR AUC SCORE: 0.3125\n",
      "Random State:  600\n",
      "Training Accuracy: 0.7954545454545454\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.5555555555555556\n",
      "PR AUC SCORE: 0.2777777777777778\n",
      "Random State:  700\n",
      "Training Accuracy: 0.7727272727272727\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.25\n",
      "Random State:  22\n",
      "Training Accuracy: 0.6363636363636364\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.25\n",
      "Mean Training Accuracy: 0.7636363636363637\n",
      "Mean Test Accuracy: 0.625\n",
      "Mean ROC AUC score: 0.5078769841269841\n",
      "Mean PR AUC score: 0.34902777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "norm_band_train_acc = []\n",
    "norm_band_test_acc = []\n",
    "norm_band_roc = []\n",
    "norm_band_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(norm_band_final, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "\n",
    "    norm_band_train_acc.append(svm_train_acc)\n",
    "    norm_band_test_acc.append(svm_test_acc)\n",
    "    norm_band_roc.append(roc)\n",
    "    norm_band_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(norm_band_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(norm_band_test_acc))\n",
    "print('Mean ROC AUC score:', mean(norm_band_roc))\n",
    "print('Mean PR AUC score:', mean(norm_band_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import PCA dataset and retain only first 6 PC's which account for 80% variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA = pd.read_csv('PCA_final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA.index = data1.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studyID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>-469.941805</td>\n",
       "      <td>0.049335</td>\n",
       "      <td>0.010597</td>\n",
       "      <td>-0.025762</td>\n",
       "      <td>0.020042</td>\n",
       "      <td>-0.004671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>8.061646</td>\n",
       "      <td>-5.761836</td>\n",
       "      <td>-4.841193</td>\n",
       "      <td>-2.171323</td>\n",
       "      <td>-2.788218</td>\n",
       "      <td>2.032909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>8.430937</td>\n",
       "      <td>6.946562</td>\n",
       "      <td>1.794457</td>\n",
       "      <td>-0.015105</td>\n",
       "      <td>-3.024961</td>\n",
       "      <td>-0.200291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>8.256683</td>\n",
       "      <td>8.288072</td>\n",
       "      <td>2.570019</td>\n",
       "      <td>-0.302641</td>\n",
       "      <td>-3.554665</td>\n",
       "      <td>4.007124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>8.376866</td>\n",
       "      <td>-0.916554</td>\n",
       "      <td>-6.212482</td>\n",
       "      <td>1.182002</td>\n",
       "      <td>-0.328640</td>\n",
       "      <td>-1.162400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>8.258802</td>\n",
       "      <td>0.870977</td>\n",
       "      <td>4.219331</td>\n",
       "      <td>0.654543</td>\n",
       "      <td>-3.580225</td>\n",
       "      <td>1.235964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>8.150815</td>\n",
       "      <td>-2.103172</td>\n",
       "      <td>-4.738280</td>\n",
       "      <td>5.019986</td>\n",
       "      <td>1.739006</td>\n",
       "      <td>-0.470654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>8.529156</td>\n",
       "      <td>-1.075416</td>\n",
       "      <td>1.555243</td>\n",
       "      <td>0.975567</td>\n",
       "      <td>2.216324</td>\n",
       "      <td>-2.077344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>8.691007</td>\n",
       "      <td>1.723237</td>\n",
       "      <td>-5.095740</td>\n",
       "      <td>-5.872935</td>\n",
       "      <td>1.253795</td>\n",
       "      <td>5.000286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>8.330590</td>\n",
       "      <td>-3.492446</td>\n",
       "      <td>-2.573330</td>\n",
       "      <td>-3.440519</td>\n",
       "      <td>0.589523</td>\n",
       "      <td>-0.811196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>8.553131</td>\n",
       "      <td>1.277476</td>\n",
       "      <td>-1.114016</td>\n",
       "      <td>-5.234553</td>\n",
       "      <td>-3.206028</td>\n",
       "      <td>-1.501332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>8.672488</td>\n",
       "      <td>0.015981</td>\n",
       "      <td>-4.417824</td>\n",
       "      <td>-4.974068</td>\n",
       "      <td>5.015134</td>\n",
       "      <td>1.444570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>8.060924</td>\n",
       "      <td>-3.404177</td>\n",
       "      <td>-1.670905</td>\n",
       "      <td>-0.015766</td>\n",
       "      <td>-0.589810</td>\n",
       "      <td>0.814938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>8.407316</td>\n",
       "      <td>2.780576</td>\n",
       "      <td>1.494663</td>\n",
       "      <td>2.738389</td>\n",
       "      <td>-0.350899</td>\n",
       "      <td>0.061606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>8.343725</td>\n",
       "      <td>-6.346800</td>\n",
       "      <td>1.870356</td>\n",
       "      <td>-2.797520</td>\n",
       "      <td>0.811928</td>\n",
       "      <td>3.526702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>8.147480</td>\n",
       "      <td>-2.233272</td>\n",
       "      <td>0.046852</td>\n",
       "      <td>2.516327</td>\n",
       "      <td>-6.168731</td>\n",
       "      <td>1.153224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>8.245402</td>\n",
       "      <td>2.583637</td>\n",
       "      <td>-5.067404</td>\n",
       "      <td>1.450122</td>\n",
       "      <td>-1.004123</td>\n",
       "      <td>-0.366070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>8.418932</td>\n",
       "      <td>-1.388080</td>\n",
       "      <td>0.822838</td>\n",
       "      <td>2.751003</td>\n",
       "      <td>0.937574</td>\n",
       "      <td>1.443123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>8.491096</td>\n",
       "      <td>-1.872720</td>\n",
       "      <td>-0.678135</td>\n",
       "      <td>-0.711563</td>\n",
       "      <td>-0.222693</td>\n",
       "      <td>-0.312042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>8.264237</td>\n",
       "      <td>7.226015</td>\n",
       "      <td>-4.778999</td>\n",
       "      <td>2.936601</td>\n",
       "      <td>0.712041</td>\n",
       "      <td>1.266246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>8.672182</td>\n",
       "      <td>1.537966</td>\n",
       "      <td>-2.587992</td>\n",
       "      <td>-3.731065</td>\n",
       "      <td>2.494876</td>\n",
       "      <td>-3.342242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>8.355781</td>\n",
       "      <td>-2.301926</td>\n",
       "      <td>-2.285624</td>\n",
       "      <td>0.911886</td>\n",
       "      <td>-1.132307</td>\n",
       "      <td>-0.483702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>8.368244</td>\n",
       "      <td>-3.130138</td>\n",
       "      <td>0.359927</td>\n",
       "      <td>0.593016</td>\n",
       "      <td>-1.534503</td>\n",
       "      <td>-1.022820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>8.590522</td>\n",
       "      <td>2.825516</td>\n",
       "      <td>1.703487</td>\n",
       "      <td>-2.263489</td>\n",
       "      <td>-0.177073</td>\n",
       "      <td>-0.566768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>8.538296</td>\n",
       "      <td>3.298215</td>\n",
       "      <td>-0.561589</td>\n",
       "      <td>1.423183</td>\n",
       "      <td>-1.070922</td>\n",
       "      <td>-0.696679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>8.201924</td>\n",
       "      <td>-3.816921</td>\n",
       "      <td>0.290515</td>\n",
       "      <td>-0.127418</td>\n",
       "      <td>-3.355348</td>\n",
       "      <td>-0.349419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>8.643114</td>\n",
       "      <td>3.246810</td>\n",
       "      <td>0.764220</td>\n",
       "      <td>0.408927</td>\n",
       "      <td>3.508107</td>\n",
       "      <td>-2.106101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>8.371865</td>\n",
       "      <td>-1.981267</td>\n",
       "      <td>-1.563791</td>\n",
       "      <td>-0.540715</td>\n",
       "      <td>-0.513858</td>\n",
       "      <td>-2.490245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>8.535858</td>\n",
       "      <td>3.013037</td>\n",
       "      <td>-0.605939</td>\n",
       "      <td>-0.291089</td>\n",
       "      <td>-0.372845</td>\n",
       "      <td>-1.107763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>8.599565</td>\n",
       "      <td>0.546161</td>\n",
       "      <td>1.997073</td>\n",
       "      <td>-3.759542</td>\n",
       "      <td>1.380745</td>\n",
       "      <td>-2.453152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>8.039447</td>\n",
       "      <td>-2.255299</td>\n",
       "      <td>-7.415057</td>\n",
       "      <td>3.669246</td>\n",
       "      <td>-1.180383</td>\n",
       "      <td>0.393992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2047</th>\n",
       "      <td>8.484428</td>\n",
       "      <td>12.066927</td>\n",
       "      <td>-1.882757</td>\n",
       "      <td>1.686828</td>\n",
       "      <td>-0.545354</td>\n",
       "      <td>0.710268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>8.432169</td>\n",
       "      <td>2.671558</td>\n",
       "      <td>0.831073</td>\n",
       "      <td>-0.666775</td>\n",
       "      <td>-0.742707</td>\n",
       "      <td>-1.167079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>8.323865</td>\n",
       "      <td>-2.300951</td>\n",
       "      <td>3.300956</td>\n",
       "      <td>-3.542203</td>\n",
       "      <td>-1.225608</td>\n",
       "      <td>-1.749491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>8.444841</td>\n",
       "      <td>-2.191852</td>\n",
       "      <td>-1.072098</td>\n",
       "      <td>0.263681</td>\n",
       "      <td>1.438553</td>\n",
       "      <td>-0.265604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>8.629302</td>\n",
       "      <td>3.695414</td>\n",
       "      <td>1.962989</td>\n",
       "      <td>-0.168602</td>\n",
       "      <td>3.104051</td>\n",
       "      <td>-1.800245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>8.264779</td>\n",
       "      <td>-3.280402</td>\n",
       "      <td>1.015337</td>\n",
       "      <td>-1.321412</td>\n",
       "      <td>0.364280</td>\n",
       "      <td>3.986855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>8.254871</td>\n",
       "      <td>-3.873659</td>\n",
       "      <td>2.332344</td>\n",
       "      <td>2.706155</td>\n",
       "      <td>1.017899</td>\n",
       "      <td>1.518916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>8.225923</td>\n",
       "      <td>-4.379280</td>\n",
       "      <td>5.395098</td>\n",
       "      <td>0.643380</td>\n",
       "      <td>-0.522846</td>\n",
       "      <td>1.641336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>8.576496</td>\n",
       "      <td>1.825386</td>\n",
       "      <td>1.830427</td>\n",
       "      <td>1.736049</td>\n",
       "      <td>1.083117</td>\n",
       "      <td>1.329922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>8.504692</td>\n",
       "      <td>2.869858</td>\n",
       "      <td>2.486749</td>\n",
       "      <td>0.181788</td>\n",
       "      <td>0.109939</td>\n",
       "      <td>-1.018897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>8.488433</td>\n",
       "      <td>2.691665</td>\n",
       "      <td>2.489616</td>\n",
       "      <td>-3.797473</td>\n",
       "      <td>-0.882744</td>\n",
       "      <td>-1.749357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>8.166598</td>\n",
       "      <td>-4.312415</td>\n",
       "      <td>-0.270006</td>\n",
       "      <td>-1.150707</td>\n",
       "      <td>-3.534556</td>\n",
       "      <td>-1.603007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>8.471067</td>\n",
       "      <td>-2.033303</td>\n",
       "      <td>2.538065</td>\n",
       "      <td>-0.638137</td>\n",
       "      <td>-0.642090</td>\n",
       "      <td>-1.409690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>8.475878</td>\n",
       "      <td>2.034554</td>\n",
       "      <td>2.083044</td>\n",
       "      <td>0.841771</td>\n",
       "      <td>0.344256</td>\n",
       "      <td>-0.219077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>7.730424</td>\n",
       "      <td>-10.018744</td>\n",
       "      <td>0.240801</td>\n",
       "      <td>6.367664</td>\n",
       "      <td>1.711156</td>\n",
       "      <td>-0.850079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>8.601290</td>\n",
       "      <td>3.542187</td>\n",
       "      <td>4.452587</td>\n",
       "      <td>2.890032</td>\n",
       "      <td>1.828924</td>\n",
       "      <td>0.131485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>8.560893</td>\n",
       "      <td>-1.594250</td>\n",
       "      <td>-1.647282</td>\n",
       "      <td>-2.723124</td>\n",
       "      <td>0.224553</td>\n",
       "      <td>-1.012382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>8.197183</td>\n",
       "      <td>1.335963</td>\n",
       "      <td>-3.625672</td>\n",
       "      <td>3.435099</td>\n",
       "      <td>-0.507488</td>\n",
       "      <td>-1.914244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>8.585552</td>\n",
       "      <td>4.997746</td>\n",
       "      <td>2.660706</td>\n",
       "      <td>1.716662</td>\n",
       "      <td>0.896149</td>\n",
       "      <td>-0.516922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>8.582159</td>\n",
       "      <td>2.156954</td>\n",
       "      <td>-0.604313</td>\n",
       "      <td>-0.555713</td>\n",
       "      <td>2.828488</td>\n",
       "      <td>2.684208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>8.157351</td>\n",
       "      <td>-5.496158</td>\n",
       "      <td>3.759084</td>\n",
       "      <td>-0.302786</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>-0.435177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>8.340156</td>\n",
       "      <td>-5.443016</td>\n",
       "      <td>3.750250</td>\n",
       "      <td>-0.556543</td>\n",
       "      <td>2.521893</td>\n",
       "      <td>0.852497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>8.602448</td>\n",
       "      <td>3.699124</td>\n",
       "      <td>1.790843</td>\n",
       "      <td>0.229875</td>\n",
       "      <td>0.397740</td>\n",
       "      <td>1.201343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>8.504195</td>\n",
       "      <td>-1.378563</td>\n",
       "      <td>2.362360</td>\n",
       "      <td>2.122315</td>\n",
       "      <td>3.813338</td>\n",
       "      <td>1.492260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>8.311902</td>\n",
       "      <td>-1.916312</td>\n",
       "      <td>0.251801</td>\n",
       "      <td>-0.553506</td>\n",
       "      <td>-1.146012</td>\n",
       "      <td>-1.740736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>8.416877</td>\n",
       "      <td>0.482019</td>\n",
       "      <td>0.276718</td>\n",
       "      <td>0.199955</td>\n",
       "      <td>1.532236</td>\n",
       "      <td>1.047101</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PC1        PC2       PC3       PC4       PC5       PC6\n",
       "studyID                                                               \n",
       "2001    -469.941805   0.049335  0.010597 -0.025762  0.020042 -0.004671\n",
       "2002       8.061646  -5.761836 -4.841193 -2.171323 -2.788218  2.032909\n",
       "2003       8.430937   6.946562  1.794457 -0.015105 -3.024961 -0.200291\n",
       "2004       8.256683   8.288072  2.570019 -0.302641 -3.554665  4.007124\n",
       "2006       8.376866  -0.916554 -6.212482  1.182002 -0.328640 -1.162400\n",
       "2008       8.258802   0.870977  4.219331  0.654543 -3.580225  1.235964\n",
       "2010       8.150815  -2.103172 -4.738280  5.019986  1.739006 -0.470654\n",
       "2012       8.529156  -1.075416  1.555243  0.975567  2.216324 -2.077344\n",
       "2013       8.691007   1.723237 -5.095740 -5.872935  1.253795  5.000286\n",
       "2014       8.330590  -3.492446 -2.573330 -3.440519  0.589523 -0.811196\n",
       "2017       8.553131   1.277476 -1.114016 -5.234553 -3.206028 -1.501332\n",
       "2021       8.672488   0.015981 -4.417824 -4.974068  5.015134  1.444570\n",
       "2022       8.060924  -3.404177 -1.670905 -0.015766 -0.589810  0.814938\n",
       "2023       8.407316   2.780576  1.494663  2.738389 -0.350899  0.061606\n",
       "2024       8.343725  -6.346800  1.870356 -2.797520  0.811928  3.526702\n",
       "2025       8.147480  -2.233272  0.046852  2.516327 -6.168731  1.153224\n",
       "2026       8.245402   2.583637 -5.067404  1.450122 -1.004123 -0.366070\n",
       "2027       8.418932  -1.388080  0.822838  2.751003  0.937574  1.443123\n",
       "2028       8.491096  -1.872720 -0.678135 -0.711563 -0.222693 -0.312042\n",
       "2029       8.264237   7.226015 -4.778999  2.936601  0.712041  1.266246\n",
       "2031       8.672182   1.537966 -2.587992 -3.731065  2.494876 -3.342242\n",
       "2032       8.355781  -2.301926 -2.285624  0.911886 -1.132307 -0.483702\n",
       "2033       8.368244  -3.130138  0.359927  0.593016 -1.534503 -1.022820\n",
       "2035       8.590522   2.825516  1.703487 -2.263489 -0.177073 -0.566768\n",
       "2037       8.538296   3.298215 -0.561589  1.423183 -1.070922 -0.696679\n",
       "2039       8.201924  -3.816921  0.290515 -0.127418 -3.355348 -0.349419\n",
       "2041       8.643114   3.246810  0.764220  0.408927  3.508107 -2.106101\n",
       "2043       8.371865  -1.981267 -1.563791 -0.540715 -0.513858 -2.490245\n",
       "2044       8.535858   3.013037 -0.605939 -0.291089 -0.372845 -1.107763\n",
       "2045       8.599565   0.546161  1.997073 -3.759542  1.380745 -2.453152\n",
       "2046       8.039447  -2.255299 -7.415057  3.669246 -1.180383  0.393992\n",
       "2047       8.484428  12.066927 -1.882757  1.686828 -0.545354  0.710268\n",
       "2048       8.432169   2.671558  0.831073 -0.666775 -0.742707 -1.167079\n",
       "2050       8.323865  -2.300951  3.300956 -3.542203 -1.225608 -1.749491\n",
       "2054       8.444841  -2.191852 -1.072098  0.263681  1.438553 -0.265604\n",
       "2055       8.629302   3.695414  1.962989 -0.168602  3.104051 -1.800245\n",
       "2056       8.264779  -3.280402  1.015337 -1.321412  0.364280  3.986855\n",
       "2057       8.254871  -3.873659  2.332344  2.706155  1.017899  1.518916\n",
       "2058       8.225923  -4.379280  5.395098  0.643380 -0.522846  1.641336\n",
       "2059       8.576496   1.825386  1.830427  1.736049  1.083117  1.329922\n",
       "2060       8.504692   2.869858  2.486749  0.181788  0.109939 -1.018897\n",
       "2063       8.488433   2.691665  2.489616 -3.797473 -0.882744 -1.749357\n",
       "2066       8.166598  -4.312415 -0.270006 -1.150707 -3.534556 -1.603007\n",
       "2067       8.471067  -2.033303  2.538065 -0.638137 -0.642090 -1.409690\n",
       "2069       8.475878   2.034554  2.083044  0.841771  0.344256 -0.219077\n",
       "2070       7.730424 -10.018744  0.240801  6.367664  1.711156 -0.850079\n",
       "2071       8.601290   3.542187  4.452587  2.890032  1.828924  0.131485\n",
       "2072       8.560893  -1.594250 -1.647282 -2.723124  0.224553 -1.012382\n",
       "2074       8.197183   1.335963 -3.625672  3.435099 -0.507488 -1.914244\n",
       "2075       8.585552   4.997746  2.660706  1.716662  0.896149 -0.516922\n",
       "2078       8.582159   2.156954 -0.604313 -0.555713  2.828488  2.684208\n",
       "2080       8.157351  -5.496158  3.759084 -0.302786  0.009967 -0.435177\n",
       "2081       8.340156  -5.443016  3.750250 -0.556543  2.521893  0.852497\n",
       "2082       8.602448   3.699124  1.790843  0.229875  0.397740  1.201343\n",
       "2083       8.504195  -1.378563  2.362360  2.122315  3.813338  1.492260\n",
       "2084       8.311902  -1.916312  0.251801 -0.553506 -1.146012 -1.740736\n",
       "2085       8.416877   0.482019  0.276718  0.199955  1.532236  1.047101"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PCA = PCA.iloc[:, 1:7]\n",
    "PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge PCA dataframe with demographic and step data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important files below:\n",
    "\n",
    "1. no_band_PCA = PCA + Demographic + stepcount as continuous variables\n",
    "2. norm_no_band_PCA = PCA + Demographic + stepcount, Normalized as continuous variables\n",
    "3. band_PCA = PCA + Demographic + stepcount as discrete variables\n",
    "4. norm_band_PCA = PCA + Demographic + stepcount, Normalized as discrete variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_band_PCA = pd.concat([PCA, data1], axis = 1, join = 'outer')\n",
    "band_PCA = pd.concat([PCA, data2], axis = 1, join = 'outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "norm = MinMaxScaler()\n",
    "\n",
    "norm_no_band_PCA = pd.DataFrame(norm.fit_transform(no_band_PCA), index = no_band_PCA.index, columns = no_band_PCA.columns.values)\n",
    "norm_band_PCA = pd.DataFrame(norm.fit_transform(band_PCA), index = band_PCA.index, columns = band_PCA.columns.values)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA + Demographic: No Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.7333333333333333\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6666666666666666\n",
      "PR AUC SCORE: 0.625\n",
      "Random State:  122\n",
      "Training Accuracy: 0.6\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.8125\n",
      "PR AUC SCORE: 0.6458333333333334\n",
      "Random State:  200\n",
      "Training Accuracy: 0.7111111111111111\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7571428571428572\n",
      "PR AUC SCORE: 0.6166666666666667\n",
      "Random State:  300\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.45714285714285713\n",
      "PR AUC SCORE: 0.4\n",
      "Random State:  368\n",
      "Training Accuracy: 0.6888888888888889\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.16666666666666666\n",
      "Random State:  400\n",
      "Training Accuracy: 0.6222222222222222\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.7777777777777778\n",
      "PR AUC SCORE: 0.42857142857142855\n",
      "Random State:  500\n",
      "Training Accuracy: 0.7555555555555555\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7500000000000002\n",
      "PR AUC SCORE: 0.6785714285714286\n",
      "Random State:  600\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5625\n",
      "PR AUC SCORE: 0.3666666666666667\n",
      "Random State:  700\n",
      "Training Accuracy: 0.7111111111111111\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.75\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  22\n",
      "Training Accuracy: 0.6888888888888889\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5857142857142856\n",
      "PR AUC SCORE: 0.4666666666666667\n",
      "Mean Training Accuracy: 0.7088888888888889\n",
      "Mean Test Accuracy: 0.65\n",
      "Mean ROC AUC score: 0.6619444444444443\n",
      "Mean PR AUC score: 0.48946428571428574\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "no_band_train_PCA = []\n",
    "no_band_test_PCA = []\n",
    "no_band_PCA_roc = []\n",
    "no_band_PCA_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(no_band_PCA, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "    \n",
    "    no_band_train_PCA.append(svm_train_acc)\n",
    "    no_band_test_PCA.append(svm_test_acc)\n",
    "    no_band_PCA_roc.append(roc)\n",
    "    no_band_PCA_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(no_band_train_PCA)) \n",
    "print('Mean Test Accuracy:', mean(no_band_test_PCA))\n",
    "print('Mean ROC AUC score:', mean(no_band_PCA_roc))\n",
    "print('Mean PR AUC score:', mean(no_band_PCA_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA + Demographic: No Band,\n",
    "Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6666666666666666\n",
      "PR AUC SCORE: 0.6666666666666667\n",
      "Random State:  122\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.625\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  200\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7\n",
      "PR AUC SCORE: 0.65\n",
      "Random State:  300\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6\n",
      "PR AUC SCORE: 0.5333333333333334\n",
      "Random State:  368\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.4\n",
      "PR AUC SCORE: 0.16666666666666666\n",
      "Random State:  400\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.4444444444444444\n",
      "PR AUC SCORE: 0.25\n",
      "Random State:  500\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  600\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.9166666666666666\n",
      "ROC AUC SCORE: 0.875\n",
      "PR AUC SCORE: 0.8333333333333334\n",
      "Random State:  700\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.4375\n",
      "PR AUC SCORE: 0.3125\n",
      "Random State:  22\n",
      "Training Accuracy: 0.7333333333333333\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7\n",
      "PR AUC SCORE: 0.65\n",
      "Mean Training Accuracy: 0.8044444444444444\n",
      "Mean Test Accuracy: 0.6833333333333333\n",
      "Mean ROC AUC score: 0.5948611111111111\n",
      "Mean PR AUC score: 0.5062500000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "norm_no_band_train_PCA = []\n",
    "norm_no_band_test_PCA = []\n",
    "norm_no_band_PCA_roc = []\n",
    "norm_no_band_PCA_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(norm_no_band_PCA, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train) \n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "\n",
    "    norm_no_band_train_PCA.append(svm_train_acc)\n",
    "    norm_no_band_test_PCA.append(svm_test_acc)\n",
    "    norm_no_band_PCA_roc.append(roc)\n",
    "    norm_no_band_PCA_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "print('Mean Training Accuracy:', mean(norm_no_band_train_PCA)) \n",
    "print('Mean Test Accuracy:', mean(norm_no_band_test_PCA)) \n",
    "print('Mean ROC AUC score:', mean(norm_no_band_PCA_roc))\n",
    "print('Mean PR AUC score:', mean(norm_no_band_PCA_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA + Demographic: Band"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.9555555555555556\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5833333333333333\n",
      "PR AUC SCORE: 0.5476190476190477\n",
      "Random State:  122\n",
      "Training Accuracy: 0.9333333333333333\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  200\n",
      "Training Accuracy: 0.7111111111111111\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.4166666666666667\n",
      "Random State:  300\n",
      "Training Accuracy: 0.7111111111111111\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.4166666666666667\n",
      "Random State:  368\n",
      "Training Accuracy: 0.6444444444444445\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.16666666666666666\n",
      "Random State:  400\n",
      "Training Accuracy: 0.6666666666666666\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.25\n",
      "Random State:  500\n",
      "Training Accuracy: 0.9777777777777777\n",
      "Test Accuracy: 0.8333333333333334\n",
      "ROC AUC SCORE: 0.8333333333333334\n",
      "PR AUC SCORE: 0.7777777777777779\n",
      "Random State:  600\n",
      "Training Accuracy: 0.6888888888888889\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  700\n",
      "Training Accuracy: 0.9555555555555556\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.4375\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  22\n",
      "Training Accuracy: 0.7111111111111111\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.4166666666666667\n",
      "Mean Training Accuracy: 0.7955555555555556\n",
      "Mean Test Accuracy: 0.6583333333333333\n",
      "Mean ROC AUC score: 0.5354166666666667\n",
      "Mean PR AUC score: 0.5062500000000001\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "band_train_PCA = []\n",
    "band_test_PCA = []\n",
    "band_PCA_roc = []\n",
    "band_PCA_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(band_PCA, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "\n",
    "    \n",
    "    band_train_PCA.append(svm_train_acc)\n",
    "    band_test_PCA.append(svm_test_acc)\n",
    "    band_PCA_roc.append(roc)\n",
    "    band_PCA_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(band_train_PCA)) \n",
    "print('Mean Test Accuracy:', mean(band_test_PCA)) \n",
    "print('Mean ROC AUC score:', mean(band_PCA_roc))\n",
    "print('Mean PR AUC score:', mean(norm_no_band_PCA_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA + Demographic: Band,\n",
    "Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State:  100\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6666666666666666\n",
      "PR AUC SCORE: 0.6666666666666667\n",
      "Random State:  122\n",
      "Training Accuracy: 0.7777777777777778\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  200\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.6\n",
      "PR AUC SCORE: 0.5333333333333334\n",
      "Random State:  300\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5285714285714286\n",
      "PR AUC SCORE: 0.43333333333333335\n",
      "Random State:  368\n",
      "Training Accuracy: 0.8666666666666667\n",
      "Test Accuracy: 0.6666666666666666\n",
      "ROC AUC SCORE: 0.4\n",
      "PR AUC SCORE: 0.16666666666666666\n",
      "Random State:  400\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.611111111111111\n",
      "PR AUC SCORE: 0.33333333333333337\n",
      "Random State:  500\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.5\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.5\n",
      "Random State:  600\n",
      "Training Accuracy: 0.8\n",
      "Test Accuracy: 0.9166666666666666\n",
      "ROC AUC SCORE: 0.875\n",
      "PR AUC SCORE: 0.8333333333333334\n",
      "Random State:  700\n",
      "Training Accuracy: 0.8444444444444444\n",
      "Test Accuracy: 0.5833333333333334\n",
      "ROC AUC SCORE: 0.5\n",
      "PR AUC SCORE: 0.3333333333333333\n",
      "Random State:  22\n",
      "Training Accuracy: 0.7333333333333333\n",
      "Test Accuracy: 0.75\n",
      "ROC AUC SCORE: 0.7\n",
      "PR AUC SCORE: 0.65\n",
      "Mean Training Accuracy: 0.8088888888888889\n",
      "Mean Test Accuracy: 0.6749999999999999\n",
      "Mean ROC AUC score: 0.5881349206349207\n",
      "Mean PR AUC score: 0.4783333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "norm_band_train_PCA = []\n",
    "norm_band_test_PCA = []\n",
    "norm_band_PCA_roc = []\n",
    "norm_band_PCA_pr = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(norm_band_PCA, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    clf = SVC(kernel = 'poly', gamma = 'scale')\n",
    "    clf.fit(X_train, Y_train)  \n",
    "    \n",
    "    X_pred = clf.predict(X_test)\n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    roc = roc_auc_score(Y_test, X_pred)\n",
    "    pr = average_precision_score(Y_test, X_pred, average = 'weighted')\n",
    "    \n",
    "    norm_band_train_PCA.append(svm_train_acc)\n",
    "    norm_band_test_PCA.append(svm_test_acc)\n",
    "    norm_band_PCA_roc.append(roc)\n",
    "    norm_band_PCA_pr.append(pr)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "    \n",
    "    print('ROC AUC SCORE:', roc)\n",
    "    print('PR AUC SCORE:', pr)\n",
    "\n",
    "\n",
    "print('Mean Training Accuracy:', mean(norm_band_train_PCA)) \n",
    "print('Mean Test Accuracy:', mean(norm_band_test_PCA)) \n",
    "print('Mean ROC AUC score:', mean(norm_band_PCA_roc))\n",
    "print('Mean PR AUC score:', mean(norm_band_PCA_pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates the model results dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = [d1_train_acc, d1_test_acc,\n",
    "                 d2_train_acc, d2_test_acc,\n",
    "                 data1_train_acc, data1_test_acc,\n",
    "                 data2_train_acc, data2_test_acc,\n",
    "                 no_band_train_acc, no_band_test_acc,\n",
    "                 norm_no_band_train_acc, norm_no_band_test_acc,\n",
    "                 band_train_acc, band_test_acc,\n",
    "                 norm_band_train_acc, norm_band_test_acc,\n",
    "                 no_band_train_PCA, no_band_test_PCA,\n",
    "                 norm_no_band_train_PCA, norm_no_band_test_PCA,\n",
    "                 band_train_PCA, band_test_PCA,\n",
    "                 norm_band_train_PCA, norm_band_test_PCA]\n",
    "\n",
    "results_roc = [d1_roc, d2_roc, data1_roc,\n",
    "                data2_roc, no_band_roc, norm_no_band_roc,\n",
    "                band_roc, norm_band_roc, no_band_PCA_roc,\n",
    "                norm_no_band_PCA_roc, band_PCA_roc, norm_band_PCA_roc]\n",
    "\n",
    "\n",
    "results_mean = [mean(d1_train_acc), mean(d1_test_acc),\n",
    "                 mean(d2_train_acc), mean(d2_test_acc),\n",
    "                 mean(data1_train_acc), mean(data1_test_acc),\n",
    "                 mean(data2_train_acc), mean(data2_test_acc),\n",
    "                 mean(no_band_train_acc), mean(no_band_test_acc),\n",
    "                 mean(norm_no_band_train_acc), mean(norm_no_band_test_acc),\n",
    "                 mean(band_train_acc), mean(band_test_acc),\n",
    "                 mean(norm_band_train_acc), mean(norm_band_test_acc),\n",
    "                 mean(no_band_train_PCA), mean(no_band_test_PCA),\n",
    "                 mean(norm_no_band_train_PCA), mean(norm_no_band_test_PCA),\n",
    "                 mean(band_train_PCA), mean(band_test_PCA),\n",
    "                 mean(norm_band_train_PCA), mean(norm_band_test_PCA)]\n",
    "\n",
    "results_roc_mean = [mean(d1_roc), mean(d2_roc), mean(data1_roc),\n",
    "                      mean(data2_roc), mean(no_band_roc), mean(norm_no_band_roc),\n",
    "                      mean(band_roc), mean(norm_band_roc), mean(no_band_PCA_roc),\n",
    "                      mean(norm_no_band_PCA_roc), mean(band_PCA_roc), mean(norm_band_PCA_roc)]\n",
    "\n",
    "\n",
    "index = ['Dem: No Band', '* Dem: No Band',\n",
    "         'Dem: Band', '* Dem: Band', \n",
    "         'Dem + Step: No Band', '* Dem + Step: No Band',\n",
    "         'Dem + Step: Band', '* Dem + Step: Band',\n",
    "         'Dem + Step + KEGG: No Band', '* Dem + Step + KEGG: No Band',\n",
    "         'Dem + Step + KEGG: No Band, Normalized', '* Dem + Step + KEGG: No Band, Normalized',\n",
    "         'Dem + Step + KEGG: Band', '* Dem + Step + KEGG: Band',\n",
    "         'Dem + Step + KEGG: Band, Normalized', '* Dem + Step + KEGG: Band, Normalized',\n",
    "         'Dem + Step + PCA: No Band', '* Dem + Step + PCA: No Band',\n",
    "         'Dem + Step + PCA: No Band, Normalized', '* Dem + Step + PCA: No Band, Normalized',\n",
    "         'Dem + Step + PCA: Band', '* Dem + Step + PCA: Band',\n",
    "         'Dem + Step + PCA: Band, Normalized', '* Dem + Step + PCA: Band, Normalized', ]\n",
    "\n",
    "roc_index = ['Dem: No Band', \n",
    "                'Dem: Band', \n",
    "                'Dem + Step: No Band',\n",
    "                'Dem + Step: Band',\n",
    "                'Dem + Step + KEGG: No Band', \n",
    "                'Dem + Step + KEGG: No Band, Normalized',\n",
    "                'Dem + Step + KEGG: Band', \n",
    "                'Dem + Step + KEGG: Band, Normalized',\n",
    "                'Dem + Step + PCA: No Band', \n",
    "                'Dem + Step + PCA: No Band, Normalized',\n",
    "                'Dem + Step + PCA: Band', \n",
    "                'Dem + Step + PCA: Band, Normalized']\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "\n",
    "svm_table = pd.DataFrame(results, columns = random_state, index = index)\n",
    "svm_table['Average Model Performance'] = results_mean\n",
    "\n",
    "svm_roc_table = pd.DataFrame(results_roc, columns = random_state, index = roc_index)\n",
    "svm_roc_table['Average ROC'] = results_roc_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_table = svm_table.T\n",
    "svm_roc_table = svm_roc_table.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svm_table shows the results for all models, the code below selects the top from each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 24)\n",
    "svm_table = svm_table.copy()\n",
    "svm_roc_table = svm_roc_table.copy()\n",
    "\n",
    "result = svm_table.loc[:,['Dem: Band', '* Dem: Band',\n",
    "                'Dem + Step: Band', '* Dem + Step: Band', \n",
    "                 'Dem + Step + KEGG: No Band', '* Dem + Step + KEGG: No Band',\n",
    "                'Dem + Step + PCA: No Band, Normalized', '* Dem + Step + PCA: No Band, Normalized']]\n",
    "\n",
    "result_roc = svm_roc_table.loc[:, roc_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dem: Band</th>\n",
       "      <th>* Dem: Band</th>\n",
       "      <th>Dem + Step: Band</th>\n",
       "      <th>* Dem + Step: Band</th>\n",
       "      <th>Dem + Step + KEGG: No Band</th>\n",
       "      <th>* Dem + Step + KEGG: No Band</th>\n",
       "      <th>Dem + Step + PCA: No Band, Normalized</th>\n",
       "      <th>* Dem + Step + PCA: No Band, Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.844444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.822222</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average Model Performance</th>\n",
       "      <td>0.797778</td>\n",
       "      <td>0.683333</td>\n",
       "      <td>0.815556</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.804444</td>\n",
       "      <td>0.683333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Dem: Band  * Dem: Band  Dem + Step: Band  \\\n",
       "100                         0.822222     0.666667          0.800000   \n",
       "122                         0.800000     0.750000          0.777778   \n",
       "200                         0.844444     0.583333          0.844444   \n",
       "300                         0.800000     0.666667          0.800000   \n",
       "368                         0.755556     0.833333          0.844444   \n",
       "400                         0.777778     0.666667          0.800000   \n",
       "500                         0.844444     0.500000          0.866667   \n",
       "600                         0.755556     0.833333          0.822222   \n",
       "700                         0.822222     0.583333          0.800000   \n",
       "22                          0.755556     0.750000          0.800000   \n",
       "Average Model Performance   0.797778     0.683333          0.815556   \n",
       "\n",
       "                           * Dem + Step: Band  Dem + Step + KEGG: No Band  \\\n",
       "100                                  0.750000                         1.0   \n",
       "122                                  0.750000                         1.0   \n",
       "200                                  0.583333                         1.0   \n",
       "300                                  0.750000                         1.0   \n",
       "368                                  0.500000                         1.0   \n",
       "400                                  0.833333                         1.0   \n",
       "500                                  0.583333                         1.0   \n",
       "600                                  0.750000                         1.0   \n",
       "700                                  0.583333                         1.0   \n",
       "22                                   0.916667                         1.0   \n",
       "Average Model Performance            0.700000                         1.0   \n",
       "\n",
       "                           * Dem + Step + KEGG: No Band  \\\n",
       "100                                            0.750000   \n",
       "122                                            0.916667   \n",
       "200                                            0.750000   \n",
       "300                                            0.750000   \n",
       "368                                            0.416667   \n",
       "400                                            0.750000   \n",
       "500                                            0.666667   \n",
       "600                                            0.583333   \n",
       "700                                            0.916667   \n",
       "22                                             0.750000   \n",
       "Average Model Performance                      0.725000   \n",
       "\n",
       "                           Dem + Step + PCA: No Band, Normalized  \\\n",
       "100                                                     0.777778   \n",
       "122                                                     0.777778   \n",
       "200                                                     0.844444   \n",
       "300                                                     0.844444   \n",
       "368                                                     0.822222   \n",
       "400                                                     0.822222   \n",
       "500                                                     0.822222   \n",
       "600                                                     0.777778   \n",
       "700                                                     0.822222   \n",
       "22                                                      0.733333   \n",
       "Average Model Performance                               0.804444   \n",
       "\n",
       "                           * Dem + Step + PCA: No Band, Normalized  \n",
       "100                                                       0.666667  \n",
       "122                                                       0.750000  \n",
       "200                                                       0.750000  \n",
       "300                                                       0.666667  \n",
       "368                                                       0.666667  \n",
       "400                                                       0.666667  \n",
       "500                                                       0.500000  \n",
       "600                                                       0.916667  \n",
       "700                                                       0.500000  \n",
       "22                                                        0.750000  \n",
       "Average Model Performance                                 0.683333  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dem: No Band</th>\n",
       "      <th>Dem: Band</th>\n",
       "      <th>Dem + Step: No Band</th>\n",
       "      <th>Dem + Step: Band</th>\n",
       "      <th>Dem + Step + KEGG: No Band</th>\n",
       "      <th>Dem + Step + KEGG: No Band, Normalized</th>\n",
       "      <th>Dem + Step + KEGG: Band</th>\n",
       "      <th>Dem + Step + KEGG: Band, Normalized</th>\n",
       "      <th>Dem + Step + PCA: No Band</th>\n",
       "      <th>Dem + Step + PCA: No Band, Normalized</th>\n",
       "      <th>Dem + Step + PCA: Band</th>\n",
       "      <th>Dem + Step + PCA: Band, Normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.528571</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.628571</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.528571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.757143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.557143</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.611111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.687500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.875000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>700</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Average ROC</th>\n",
       "      <td>0.526429</td>\n",
       "      <td>0.581468</td>\n",
       "      <td>0.557639</td>\n",
       "      <td>0.630357</td>\n",
       "      <td>0.751706</td>\n",
       "      <td>0.514127</td>\n",
       "      <td>0.580754</td>\n",
       "      <td>0.507877</td>\n",
       "      <td>0.661944</td>\n",
       "      <td>0.594861</td>\n",
       "      <td>0.535417</td>\n",
       "      <td>0.588135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Dem: No Band  Dem: Band  Dem + Step: No Band  Dem + Step: Band  \\\n",
       "100              0.666667   0.666667             0.583333          0.750000   \n",
       "122              0.500000   0.625000             0.562500          0.625000   \n",
       "200              0.285714   0.500000             0.457143          0.528571   \n",
       "300              0.728571   0.628571             0.457143          0.700000   \n",
       "368              0.300000   0.500000             0.250000          0.300000   \n",
       "400              0.333333   0.444444             0.722222          0.666667   \n",
       "500              0.500000   0.500000             0.583333          0.583333   \n",
       "600              0.750000   0.812500             0.625000          0.750000   \n",
       "700              0.500000   0.437500             0.750000          0.500000   \n",
       "22               0.700000   0.700000             0.585714          0.900000   \n",
       "Average ROC      0.526429   0.581468             0.557639          0.630357   \n",
       "\n",
       "             Dem + Step + KEGG: No Band  \\\n",
       "100                            0.750000   \n",
       "122                            0.875000   \n",
       "200                            0.728571   \n",
       "300                            0.728571   \n",
       "368                            0.650000   \n",
       "400                            0.757143   \n",
       "500                            0.750000   \n",
       "600                            0.611111   \n",
       "700                            0.944444   \n",
       "22                             0.722222   \n",
       "Average ROC                    0.751706   \n",
       "\n",
       "             Dem + Step + KEGG: No Band, Normalized  Dem + Step + KEGG: Band  \\\n",
       "100                                        0.500000                 0.666667   \n",
       "122                                        0.625000                 0.687500   \n",
       "200                                        0.457143                 0.557143   \n",
       "300                                        0.500000                 0.657143   \n",
       "368                                        0.700000                 0.550000   \n",
       "400                                        0.428571                 0.557143   \n",
       "500                                        0.375000                 0.687500   \n",
       "600                                        0.555556                 0.500000   \n",
       "700                                        0.500000                 0.500000   \n",
       "22                                         0.500000                 0.444444   \n",
       "Average ROC                                0.514127                 0.580754   \n",
       "\n",
       "             Dem + Step + KEGG: Band, Normalized  Dem + Step + PCA: No Band  \\\n",
       "100                                     0.500000                   0.666667   \n",
       "122                                     0.500000                   0.812500   \n",
       "200                                     0.457143                   0.757143   \n",
       "300                                     0.500000                   0.457143   \n",
       "368                                     0.700000                   0.500000   \n",
       "400                                     0.428571                   0.777778   \n",
       "500                                     0.437500                   0.750000   \n",
       "600                                     0.555556                   0.562500   \n",
       "700                                     0.500000                   0.750000   \n",
       "22                                      0.500000                   0.585714   \n",
       "Average ROC                             0.507877                   0.661944   \n",
       "\n",
       "             Dem + Step + PCA: No Band, Normalized  Dem + Step + PCA: Band  \\\n",
       "100                                       0.666667                0.583333   \n",
       "122                                       0.625000                0.500000   \n",
       "200                                       0.700000                0.500000   \n",
       "300                                       0.600000                0.500000   \n",
       "368                                       0.400000                0.500000   \n",
       "400                                       0.444444                0.500000   \n",
       "500                                       0.500000                0.833333   \n",
       "600                                       0.875000                0.500000   \n",
       "700                                       0.437500                0.437500   \n",
       "22                                        0.700000                0.500000   \n",
       "Average ROC                               0.594861                0.535417   \n",
       "\n",
       "             Dem + Step + PCA: Band, Normalized  \n",
       "100                                    0.666667  \n",
       "122                                    0.500000  \n",
       "200                                    0.600000  \n",
       "300                                    0.528571  \n",
       "368                                    0.400000  \n",
       "400                                    0.611111  \n",
       "500                                    0.500000  \n",
       "600                                    0.875000  \n",
       "700                                    0.500000  \n",
       "22                                     0.700000  \n",
       "Average ROC                            0.588135  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_excel(\"svm_model_results.xlsx\")\n",
    "result_roc.to_excel(\"svm_roc_results.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extension from original work "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = no_band_final.drop('Weight_loss_band', axis = 1)\n",
    "y = no_band_final[['Weight_loss_band']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hsa03040.mean</th>\n",
       "      <th>hsa03040.var</th>\n",
       "      <th>hsa03050.mean</th>\n",
       "      <th>hsa03050.var</th>\n",
       "      <th>hsa03060.mean</th>\n",
       "      <th>hsa03060.var</th>\n",
       "      <th>hsa04130.mean</th>\n",
       "      <th>hsa04130.var</th>\n",
       "      <th>hsa04141.mean</th>\n",
       "      <th>hsa04141.var</th>\n",
       "      <th>hsa04662.mean</th>\n",
       "      <th>hsa04662.var</th>\n",
       "      <th>Group</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>BMI</th>\n",
       "      <th>Steps</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>studyID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2001</th>\n",
       "      <td>86.154219</td>\n",
       "      <td>21800.778325</td>\n",
       "      <td>79.794512</td>\n",
       "      <td>7178.102233</td>\n",
       "      <td>72.230671</td>\n",
       "      <td>3886.474530</td>\n",
       "      <td>50.664777</td>\n",
       "      <td>1633.094649</td>\n",
       "      <td>98.424988</td>\n",
       "      <td>49954.700683</td>\n",
       "      <td>159.915149</td>\n",
       "      <td>37825.957918</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>36.24</td>\n",
       "      <td>10723.455556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2002</th>\n",
       "      <td>97.584318</td>\n",
       "      <td>15007.706572</td>\n",
       "      <td>68.788322</td>\n",
       "      <td>3862.390316</td>\n",
       "      <td>101.742322</td>\n",
       "      <td>5646.975137</td>\n",
       "      <td>43.465866</td>\n",
       "      <td>1679.074231</td>\n",
       "      <td>84.057955</td>\n",
       "      <td>15199.893980</td>\n",
       "      <td>102.296825</td>\n",
       "      <td>9691.314802</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>35.71</td>\n",
       "      <td>4654.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003</th>\n",
       "      <td>95.057381</td>\n",
       "      <td>12617.681555</td>\n",
       "      <td>68.462327</td>\n",
       "      <td>3563.701033</td>\n",
       "      <td>102.477762</td>\n",
       "      <td>11260.700282</td>\n",
       "      <td>49.024537</td>\n",
       "      <td>1575.377166</td>\n",
       "      <td>82.168618</td>\n",
       "      <td>11999.813861</td>\n",
       "      <td>106.311380</td>\n",
       "      <td>13742.844297</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>42.80</td>\n",
       "      <td>5675.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004</th>\n",
       "      <td>88.907581</td>\n",
       "      <td>18117.487593</td>\n",
       "      <td>82.024351</td>\n",
       "      <td>7074.260758</td>\n",
       "      <td>82.144727</td>\n",
       "      <td>4236.446791</td>\n",
       "      <td>48.366668</td>\n",
       "      <td>2024.659491</td>\n",
       "      <td>97.786337</td>\n",
       "      <td>50042.642204</td>\n",
       "      <td>127.510289</td>\n",
       "      <td>20393.076993</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>26.72</td>\n",
       "      <td>9855.843373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>84.981499</td>\n",
       "      <td>9910.820112</td>\n",
       "      <td>79.067493</td>\n",
       "      <td>6394.689793</td>\n",
       "      <td>96.999589</td>\n",
       "      <td>9473.002696</td>\n",
       "      <td>48.343406</td>\n",
       "      <td>1081.223172</td>\n",
       "      <td>75.811154</td>\n",
       "      <td>9582.504013</td>\n",
       "      <td>129.117374</td>\n",
       "      <td>18994.681879</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>43.89</td>\n",
       "      <td>7595.388889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2008</th>\n",
       "      <td>82.929690</td>\n",
       "      <td>11331.997415</td>\n",
       "      <td>66.657281</td>\n",
       "      <td>4586.447542</td>\n",
       "      <td>79.140014</td>\n",
       "      <td>4713.764474</td>\n",
       "      <td>44.445032</td>\n",
       "      <td>1542.721675</td>\n",
       "      <td>92.440769</td>\n",
       "      <td>35019.500296</td>\n",
       "      <td>128.875116</td>\n",
       "      <td>21765.144484</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>29.81</td>\n",
       "      <td>13632.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>85.355635</td>\n",
       "      <td>11538.139061</td>\n",
       "      <td>73.918468</td>\n",
       "      <td>7730.427403</td>\n",
       "      <td>70.030188</td>\n",
       "      <td>2029.684544</td>\n",
       "      <td>43.418717</td>\n",
       "      <td>989.941485</td>\n",
       "      <td>75.195009</td>\n",
       "      <td>12947.641050</td>\n",
       "      <td>121.467418</td>\n",
       "      <td>17934.217227</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>26.33</td>\n",
       "      <td>6268.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>79.392239</td>\n",
       "      <td>11264.853351</td>\n",
       "      <td>71.502587</td>\n",
       "      <td>4623.222450</td>\n",
       "      <td>85.543070</td>\n",
       "      <td>8752.621082</td>\n",
       "      <td>41.999912</td>\n",
       "      <td>697.849831</td>\n",
       "      <td>90.236760</td>\n",
       "      <td>39247.843464</td>\n",
       "      <td>132.935727</td>\n",
       "      <td>27016.620548</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>24.84</td>\n",
       "      <td>10499.788462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013</th>\n",
       "      <td>78.975281</td>\n",
       "      <td>11137.403893</td>\n",
       "      <td>75.901554</td>\n",
       "      <td>6313.789709</td>\n",
       "      <td>74.507146</td>\n",
       "      <td>3365.581198</td>\n",
       "      <td>49.651971</td>\n",
       "      <td>993.962174</td>\n",
       "      <td>84.045117</td>\n",
       "      <td>13879.697868</td>\n",
       "      <td>140.987077</td>\n",
       "      <td>25844.145886</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>1</td>\n",
       "      <td>27.68</td>\n",
       "      <td>4357.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014</th>\n",
       "      <td>87.986462</td>\n",
       "      <td>15223.094407</td>\n",
       "      <td>66.645613</td>\n",
       "      <td>2980.177737</td>\n",
       "      <td>94.312513</td>\n",
       "      <td>8191.870489</td>\n",
       "      <td>43.780435</td>\n",
       "      <td>872.193056</td>\n",
       "      <td>89.732871</td>\n",
       "      <td>32561.387584</td>\n",
       "      <td>124.160901</td>\n",
       "      <td>18185.938066</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>28.03</td>\n",
       "      <td>9324.196429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>77.755436</td>\n",
       "      <td>10616.639173</td>\n",
       "      <td>71.696635</td>\n",
       "      <td>4116.902398</td>\n",
       "      <td>77.247865</td>\n",
       "      <td>7836.542319</td>\n",
       "      <td>45.797804</td>\n",
       "      <td>1360.120681</td>\n",
       "      <td>77.455720</td>\n",
       "      <td>16302.483979</td>\n",
       "      <td>120.939594</td>\n",
       "      <td>21636.959062</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>28.58</td>\n",
       "      <td>10186.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>84.305451</td>\n",
       "      <td>19095.821110</td>\n",
       "      <td>78.528183</td>\n",
       "      <td>6753.366277</td>\n",
       "      <td>96.205240</td>\n",
       "      <td>18887.341459</td>\n",
       "      <td>45.270453</td>\n",
       "      <td>1189.128157</td>\n",
       "      <td>94.273918</td>\n",
       "      <td>19673.172826</td>\n",
       "      <td>136.280128</td>\n",
       "      <td>23509.834271</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>28.90</td>\n",
       "      <td>5194.886364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>89.930269</td>\n",
       "      <td>11210.028770</td>\n",
       "      <td>76.787824</td>\n",
       "      <td>6479.607009</td>\n",
       "      <td>91.144792</td>\n",
       "      <td>7384.220616</td>\n",
       "      <td>45.243613</td>\n",
       "      <td>937.190014</td>\n",
       "      <td>83.561518</td>\n",
       "      <td>21902.492550</td>\n",
       "      <td>115.160412</td>\n",
       "      <td>16076.004261</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>26.05</td>\n",
       "      <td>11843.067416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>80.794456</td>\n",
       "      <td>13378.032499</td>\n",
       "      <td>71.638355</td>\n",
       "      <td>6861.057579</td>\n",
       "      <td>68.517151</td>\n",
       "      <td>4977.711932</td>\n",
       "      <td>49.310246</td>\n",
       "      <td>1507.653136</td>\n",
       "      <td>70.188524</td>\n",
       "      <td>9720.854821</td>\n",
       "      <td>147.573828</td>\n",
       "      <td>28635.270637</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>30.17</td>\n",
       "      <td>12333.911111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>88.300963</td>\n",
       "      <td>16023.856548</td>\n",
       "      <td>77.164995</td>\n",
       "      <td>5786.831748</td>\n",
       "      <td>96.027550</td>\n",
       "      <td>6805.130877</td>\n",
       "      <td>50.184482</td>\n",
       "      <td>1706.580490</td>\n",
       "      <td>89.694460</td>\n",
       "      <td>21883.962547</td>\n",
       "      <td>121.631534</td>\n",
       "      <td>19238.969494</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>27.83</td>\n",
       "      <td>13218.820225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>89.899460</td>\n",
       "      <td>13346.454263</td>\n",
       "      <td>74.136285</td>\n",
       "      <td>4309.389757</td>\n",
       "      <td>87.456163</td>\n",
       "      <td>4889.118972</td>\n",
       "      <td>47.655724</td>\n",
       "      <td>1490.806989</td>\n",
       "      <td>87.966418</td>\n",
       "      <td>31171.890319</td>\n",
       "      <td>117.686425</td>\n",
       "      <td>18595.445416</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>28.37</td>\n",
       "      <td>11552.678161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>90.063112</td>\n",
       "      <td>14508.418809</td>\n",
       "      <td>66.363304</td>\n",
       "      <td>3393.824978</td>\n",
       "      <td>89.123116</td>\n",
       "      <td>8152.698244</td>\n",
       "      <td>51.435005</td>\n",
       "      <td>1723.421767</td>\n",
       "      <td>80.378923</td>\n",
       "      <td>16456.469727</td>\n",
       "      <td>128.433052</td>\n",
       "      <td>20875.689466</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>28.56</td>\n",
       "      <td>18288.966667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>87.965760</td>\n",
       "      <td>15990.147424</td>\n",
       "      <td>78.535297</td>\n",
       "      <td>6267.562339</td>\n",
       "      <td>87.762394</td>\n",
       "      <td>4773.675717</td>\n",
       "      <td>45.369075</td>\n",
       "      <td>1067.351096</td>\n",
       "      <td>84.632726</td>\n",
       "      <td>14455.992459</td>\n",
       "      <td>135.383158</td>\n",
       "      <td>22817.918896</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>29.34</td>\n",
       "      <td>5516.397590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>93.677503</td>\n",
       "      <td>13275.656212</td>\n",
       "      <td>70.778871</td>\n",
       "      <td>3471.347787</td>\n",
       "      <td>104.425612</td>\n",
       "      <td>14447.066268</td>\n",
       "      <td>45.469176</td>\n",
       "      <td>1438.612528</td>\n",
       "      <td>98.088122</td>\n",
       "      <td>44218.740718</td>\n",
       "      <td>104.052830</td>\n",
       "      <td>13351.055102</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>32.68</td>\n",
       "      <td>6424.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>86.837443</td>\n",
       "      <td>12777.916452</td>\n",
       "      <td>79.732441</td>\n",
       "      <td>6472.807282</td>\n",
       "      <td>79.335992</td>\n",
       "      <td>3034.885833</td>\n",
       "      <td>42.800341</td>\n",
       "      <td>813.354228</td>\n",
       "      <td>77.095396</td>\n",
       "      <td>12954.634462</td>\n",
       "      <td>113.800537</td>\n",
       "      <td>16342.677332</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>27.19</td>\n",
       "      <td>10695.822222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>83.304774</td>\n",
       "      <td>13847.286350</td>\n",
       "      <td>87.231774</td>\n",
       "      <td>9472.004453</td>\n",
       "      <td>81.831085</td>\n",
       "      <td>3198.271171</td>\n",
       "      <td>43.625473</td>\n",
       "      <td>1359.819042</td>\n",
       "      <td>87.186804</td>\n",
       "      <td>25676.322767</td>\n",
       "      <td>135.052200</td>\n",
       "      <td>24490.830479</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>30.52</td>\n",
       "      <td>11053.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>84.599143</td>\n",
       "      <td>15076.090800</td>\n",
       "      <td>77.215088</td>\n",
       "      <td>6320.330961</td>\n",
       "      <td>84.280116</td>\n",
       "      <td>4353.298933</td>\n",
       "      <td>50.953525</td>\n",
       "      <td>1561.893886</td>\n",
       "      <td>85.047436</td>\n",
       "      <td>21987.069585</td>\n",
       "      <td>134.926364</td>\n",
       "      <td>22148.317176</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>32.30</td>\n",
       "      <td>13450.933333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>91.443304</td>\n",
       "      <td>14426.500034</td>\n",
       "      <td>74.061203</td>\n",
       "      <td>4653.030840</td>\n",
       "      <td>97.494060</td>\n",
       "      <td>8429.803001</td>\n",
       "      <td>42.320264</td>\n",
       "      <td>740.599363</td>\n",
       "      <td>77.482798</td>\n",
       "      <td>10198.057425</td>\n",
       "      <td>120.576410</td>\n",
       "      <td>16771.091209</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>33.59</td>\n",
       "      <td>9783.255556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2035</th>\n",
       "      <td>89.742029</td>\n",
       "      <td>13641.656921</td>\n",
       "      <td>68.352144</td>\n",
       "      <td>4025.952556</td>\n",
       "      <td>84.795618</td>\n",
       "      <td>5588.773860</td>\n",
       "      <td>43.415356</td>\n",
       "      <td>1220.701921</td>\n",
       "      <td>81.904858</td>\n",
       "      <td>15534.413694</td>\n",
       "      <td>114.604942</td>\n",
       "      <td>13748.974679</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>31.41</td>\n",
       "      <td>12545.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2037</th>\n",
       "      <td>88.493816</td>\n",
       "      <td>18977.480431</td>\n",
       "      <td>78.903030</td>\n",
       "      <td>6658.529322</td>\n",
       "      <td>86.744563</td>\n",
       "      <td>4871.696860</td>\n",
       "      <td>47.545816</td>\n",
       "      <td>1473.574541</td>\n",
       "      <td>89.300040</td>\n",
       "      <td>29976.301936</td>\n",
       "      <td>134.420417</td>\n",
       "      <td>23353.288066</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>28.51</td>\n",
       "      <td>10382.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>93.624940</td>\n",
       "      <td>15353.276799</td>\n",
       "      <td>73.941307</td>\n",
       "      <td>4836.814917</td>\n",
       "      <td>89.770629</td>\n",
       "      <td>6283.943022</td>\n",
       "      <td>40.108883</td>\n",
       "      <td>864.476649</td>\n",
       "      <td>77.506434</td>\n",
       "      <td>8752.024963</td>\n",
       "      <td>112.076397</td>\n",
       "      <td>15239.603971</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>28.99</td>\n",
       "      <td>9360.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>80.031678</td>\n",
       "      <td>13067.512235</td>\n",
       "      <td>90.442608</td>\n",
       "      <td>10003.348912</td>\n",
       "      <td>83.743442</td>\n",
       "      <td>5018.654118</td>\n",
       "      <td>43.492050</td>\n",
       "      <td>1030.933741</td>\n",
       "      <td>82.609518</td>\n",
       "      <td>19682.190262</td>\n",
       "      <td>131.169521</td>\n",
       "      <td>21764.476968</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>1</td>\n",
       "      <td>28.52</td>\n",
       "      <td>8925.269663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>86.465604</td>\n",
       "      <td>12329.403094</td>\n",
       "      <td>75.059489</td>\n",
       "      <td>5550.219294</td>\n",
       "      <td>93.121017</td>\n",
       "      <td>7637.048792</td>\n",
       "      <td>43.252300</td>\n",
       "      <td>1110.147691</td>\n",
       "      <td>79.365612</td>\n",
       "      <td>15510.010492</td>\n",
       "      <td>121.598649</td>\n",
       "      <td>17240.654659</td>\n",
       "      <td>1</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>26.63</td>\n",
       "      <td>7866.140845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>89.553404</td>\n",
       "      <td>14247.092850</td>\n",
       "      <td>82.023975</td>\n",
       "      <td>5922.234284</td>\n",
       "      <td>87.031223</td>\n",
       "      <td>4133.966007</td>\n",
       "      <td>42.387568</td>\n",
       "      <td>781.423972</td>\n",
       "      <td>71.818587</td>\n",
       "      <td>5492.976344</td>\n",
       "      <td>126.783921</td>\n",
       "      <td>17723.167896</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>28.50</td>\n",
       "      <td>7828.922222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2045</th>\n",
       "      <td>82.970524</td>\n",
       "      <td>12717.350417</td>\n",
       "      <td>74.575575</td>\n",
       "      <td>6783.391260</td>\n",
       "      <td>85.570066</td>\n",
       "      <td>7133.801809</td>\n",
       "      <td>46.162720</td>\n",
       "      <td>1430.185268</td>\n",
       "      <td>97.719079</td>\n",
       "      <td>61376.614054</td>\n",
       "      <td>134.111570</td>\n",
       "      <td>25805.889655</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>26.42</td>\n",
       "      <td>7802.383721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>99.106002</td>\n",
       "      <td>13132.237552</td>\n",
       "      <td>68.486609</td>\n",
       "      <td>3142.365681</td>\n",
       "      <td>100.022804</td>\n",
       "      <td>6862.608329</td>\n",
       "      <td>40.511760</td>\n",
       "      <td>691.291473</td>\n",
       "      <td>87.248678</td>\n",
       "      <td>18915.603559</td>\n",
       "      <td>99.686216</td>\n",
       "      <td>11418.570726</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>25.03</td>\n",
       "      <td>6550.046512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2048</th>\n",
       "      <td>90.142907</td>\n",
       "      <td>12099.007956</td>\n",
       "      <td>73.615715</td>\n",
       "      <td>5697.750368</td>\n",
       "      <td>97.552240</td>\n",
       "      <td>8180.729473</td>\n",
       "      <td>44.508669</td>\n",
       "      <td>795.479967</td>\n",
       "      <td>76.494275</td>\n",
       "      <td>9859.374586</td>\n",
       "      <td>115.196916</td>\n",
       "      <td>14010.076171</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>17.71</td>\n",
       "      <td>10610.886364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2050</th>\n",
       "      <td>92.261336</td>\n",
       "      <td>16844.280401</td>\n",
       "      <td>74.742986</td>\n",
       "      <td>7379.447844</td>\n",
       "      <td>84.651212</td>\n",
       "      <td>5783.691802</td>\n",
       "      <td>48.895375</td>\n",
       "      <td>1368.203442</td>\n",
       "      <td>72.303895</td>\n",
       "      <td>5401.761608</td>\n",
       "      <td>144.392211</td>\n",
       "      <td>27967.615539</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>24.17</td>\n",
       "      <td>9727.197674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2054</th>\n",
       "      <td>89.731348</td>\n",
       "      <td>20029.907545</td>\n",
       "      <td>65.600004</td>\n",
       "      <td>5081.444090</td>\n",
       "      <td>70.618649</td>\n",
       "      <td>3169.815461</td>\n",
       "      <td>47.018640</td>\n",
       "      <td>1328.109248</td>\n",
       "      <td>85.070438</td>\n",
       "      <td>24412.719845</td>\n",
       "      <td>142.054988</td>\n",
       "      <td>27701.952087</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>27.48</td>\n",
       "      <td>10792.277778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2055</th>\n",
       "      <td>96.257339</td>\n",
       "      <td>13929.679246</td>\n",
       "      <td>71.490377</td>\n",
       "      <td>3893.740275</td>\n",
       "      <td>91.675426</td>\n",
       "      <td>5426.960873</td>\n",
       "      <td>46.023319</td>\n",
       "      <td>959.155122</td>\n",
       "      <td>78.144561</td>\n",
       "      <td>7950.437283</td>\n",
       "      <td>120.653457</td>\n",
       "      <td>18534.375546</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>22.46</td>\n",
       "      <td>8528.325581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2056</th>\n",
       "      <td>83.867171</td>\n",
       "      <td>14193.854171</td>\n",
       "      <td>71.015505</td>\n",
       "      <td>5074.283124</td>\n",
       "      <td>87.108616</td>\n",
       "      <td>13083.056579</td>\n",
       "      <td>53.392858</td>\n",
       "      <td>1728.275747</td>\n",
       "      <td>76.659719</td>\n",
       "      <td>8163.592298</td>\n",
       "      <td>146.430768</td>\n",
       "      <td>29788.762730</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>28.29</td>\n",
       "      <td>12033.955556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2057</th>\n",
       "      <td>90.695080</td>\n",
       "      <td>16151.968610</td>\n",
       "      <td>75.381286</td>\n",
       "      <td>6656.041090</td>\n",
       "      <td>84.650089</td>\n",
       "      <td>7530.567704</td>\n",
       "      <td>51.780389</td>\n",
       "      <td>1815.543035</td>\n",
       "      <td>85.988297</td>\n",
       "      <td>19463.255531</td>\n",
       "      <td>141.308048</td>\n",
       "      <td>27256.647749</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>24.26</td>\n",
       "      <td>14191.677778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2058</th>\n",
       "      <td>91.945818</td>\n",
       "      <td>16277.789919</td>\n",
       "      <td>69.678581</td>\n",
       "      <td>4732.540215</td>\n",
       "      <td>86.962484</td>\n",
       "      <td>7390.052685</td>\n",
       "      <td>51.739166</td>\n",
       "      <td>1464.739633</td>\n",
       "      <td>79.651849</td>\n",
       "      <td>14018.276563</td>\n",
       "      <td>147.583343</td>\n",
       "      <td>29334.118119</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>37.55</td>\n",
       "      <td>3087.068182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2059</th>\n",
       "      <td>88.161458</td>\n",
       "      <td>14606.067906</td>\n",
       "      <td>66.979994</td>\n",
       "      <td>3673.307013</td>\n",
       "      <td>80.762856</td>\n",
       "      <td>6687.343045</td>\n",
       "      <td>47.626025</td>\n",
       "      <td>1262.257970</td>\n",
       "      <td>78.271969</td>\n",
       "      <td>12096.209910</td>\n",
       "      <td>126.417151</td>\n",
       "      <td>19818.482358</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>30.08</td>\n",
       "      <td>10746.465909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2060</th>\n",
       "      <td>93.574857</td>\n",
       "      <td>15257.636477</td>\n",
       "      <td>71.204814</td>\n",
       "      <td>3909.931936</td>\n",
       "      <td>94.814563</td>\n",
       "      <td>5031.257154</td>\n",
       "      <td>46.736111</td>\n",
       "      <td>1515.774999</td>\n",
       "      <td>79.785701</td>\n",
       "      <td>10052.618329</td>\n",
       "      <td>127.165161</td>\n",
       "      <td>20044.039832</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>23.55</td>\n",
       "      <td>7947.636364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2063</th>\n",
       "      <td>89.245037</td>\n",
       "      <td>11176.705766</td>\n",
       "      <td>70.628577</td>\n",
       "      <td>3347.309704</td>\n",
       "      <td>95.430374</td>\n",
       "      <td>9385.516058</td>\n",
       "      <td>41.538239</td>\n",
       "      <td>469.388352</td>\n",
       "      <td>76.277565</td>\n",
       "      <td>7148.296414</td>\n",
       "      <td>116.229302</td>\n",
       "      <td>14822.815296</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>37.58</td>\n",
       "      <td>7116.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2066</th>\n",
       "      <td>84.904408</td>\n",
       "      <td>16842.328607</td>\n",
       "      <td>71.120997</td>\n",
       "      <td>6456.650526</td>\n",
       "      <td>79.107568</td>\n",
       "      <td>4211.791912</td>\n",
       "      <td>47.927609</td>\n",
       "      <td>1057.170103</td>\n",
       "      <td>87.973484</td>\n",
       "      <td>26837.327336</td>\n",
       "      <td>139.761651</td>\n",
       "      <td>28263.382365</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>27.75</td>\n",
       "      <td>11816.044444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2067</th>\n",
       "      <td>85.165135</td>\n",
       "      <td>13255.925650</td>\n",
       "      <td>73.960563</td>\n",
       "      <td>6143.702706</td>\n",
       "      <td>80.197836</td>\n",
       "      <td>4663.151581</td>\n",
       "      <td>51.550945</td>\n",
       "      <td>1208.524718</td>\n",
       "      <td>79.389044</td>\n",
       "      <td>11722.503280</td>\n",
       "      <td>140.346213</td>\n",
       "      <td>24172.835629</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>28.70</td>\n",
       "      <td>3800.273973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>88.345350</td>\n",
       "      <td>12101.396712</td>\n",
       "      <td>68.961044</td>\n",
       "      <td>4033.150688</td>\n",
       "      <td>90.122411</td>\n",
       "      <td>7503.729522</td>\n",
       "      <td>47.301401</td>\n",
       "      <td>973.658611</td>\n",
       "      <td>81.623828</td>\n",
       "      <td>16475.039268</td>\n",
       "      <td>121.649818</td>\n",
       "      <td>17242.368864</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>28.70</td>\n",
       "      <td>13101.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>80.465605</td>\n",
       "      <td>15969.087581</td>\n",
       "      <td>87.158230</td>\n",
       "      <td>15931.926789</td>\n",
       "      <td>71.814622</td>\n",
       "      <td>3428.432314</td>\n",
       "      <td>56.186365</td>\n",
       "      <td>1754.897754</td>\n",
       "      <td>95.326720</td>\n",
       "      <td>34261.002217</td>\n",
       "      <td>164.485529</td>\n",
       "      <td>44005.384666</td>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>35.36</td>\n",
       "      <td>10405.622222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>94.909795</td>\n",
       "      <td>12911.268116</td>\n",
       "      <td>64.009146</td>\n",
       "      <td>2963.523683</td>\n",
       "      <td>100.214998</td>\n",
       "      <td>12428.768562</td>\n",
       "      <td>46.198503</td>\n",
       "      <td>1130.240997</td>\n",
       "      <td>75.687219</td>\n",
       "      <td>7265.876517</td>\n",
       "      <td>121.131946</td>\n",
       "      <td>15827.414438</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>37.34</td>\n",
       "      <td>9117.604651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>92.760542</td>\n",
       "      <td>16757.182832</td>\n",
       "      <td>73.523155</td>\n",
       "      <td>4477.140378</td>\n",
       "      <td>88.223900</td>\n",
       "      <td>4594.908620</td>\n",
       "      <td>42.113304</td>\n",
       "      <td>806.445894</td>\n",
       "      <td>80.784976</td>\n",
       "      <td>11308.600052</td>\n",
       "      <td>129.751098</td>\n",
       "      <td>22728.937153</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>27.46</td>\n",
       "      <td>8744.563218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>83.118367</td>\n",
       "      <td>10945.902014</td>\n",
       "      <td>74.709817</td>\n",
       "      <td>4991.980685</td>\n",
       "      <td>86.465732</td>\n",
       "      <td>6730.052489</td>\n",
       "      <td>42.295604</td>\n",
       "      <td>1381.937235</td>\n",
       "      <td>89.948690</td>\n",
       "      <td>39501.124399</td>\n",
       "      <td>111.804078</td>\n",
       "      <td>13088.737936</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>21.63</td>\n",
       "      <td>7098.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>94.319425</td>\n",
       "      <td>12512.572591</td>\n",
       "      <td>73.736107</td>\n",
       "      <td>3746.418588</td>\n",
       "      <td>102.554280</td>\n",
       "      <td>9611.344926</td>\n",
       "      <td>42.600133</td>\n",
       "      <td>1030.573381</td>\n",
       "      <td>74.983478</td>\n",
       "      <td>7150.664821</td>\n",
       "      <td>109.713684</td>\n",
       "      <td>14285.898424</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>31.62</td>\n",
       "      <td>9615.105882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2078</th>\n",
       "      <td>85.742390</td>\n",
       "      <td>12729.406681</td>\n",
       "      <td>70.930927</td>\n",
       "      <td>4162.966425</td>\n",
       "      <td>75.298183</td>\n",
       "      <td>5966.647276</td>\n",
       "      <td>42.831349</td>\n",
       "      <td>1022.180817</td>\n",
       "      <td>75.304986</td>\n",
       "      <td>9978.306786</td>\n",
       "      <td>120.234976</td>\n",
       "      <td>16627.451104</td>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>24.26</td>\n",
       "      <td>11135.222222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2080</th>\n",
       "      <td>86.098120</td>\n",
       "      <td>16357.847727</td>\n",
       "      <td>70.979893</td>\n",
       "      <td>7001.641242</td>\n",
       "      <td>71.187518</td>\n",
       "      <td>3715.127292</td>\n",
       "      <td>48.427998</td>\n",
       "      <td>1643.900517</td>\n",
       "      <td>74.891012</td>\n",
       "      <td>9204.929276</td>\n",
       "      <td>158.004802</td>\n",
       "      <td>35318.290323</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>30.76</td>\n",
       "      <td>7823.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2081</th>\n",
       "      <td>94.336278</td>\n",
       "      <td>19677.124463</td>\n",
       "      <td>77.345154</td>\n",
       "      <td>7144.937707</td>\n",
       "      <td>81.063363</td>\n",
       "      <td>6756.216948</td>\n",
       "      <td>50.198273</td>\n",
       "      <td>1480.592118</td>\n",
       "      <td>77.077226</td>\n",
       "      <td>11062.048165</td>\n",
       "      <td>152.984761</td>\n",
       "      <td>33089.092366</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>31.08</td>\n",
       "      <td>11297.533333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2082</th>\n",
       "      <td>87.532946</td>\n",
       "      <td>10669.121344</td>\n",
       "      <td>69.246463</td>\n",
       "      <td>4164.073298</td>\n",
       "      <td>100.206618</td>\n",
       "      <td>11937.233087</td>\n",
       "      <td>46.501057</td>\n",
       "      <td>1330.750730</td>\n",
       "      <td>77.054105</td>\n",
       "      <td>10045.199240</td>\n",
       "      <td>112.856439</td>\n",
       "      <td>13096.204865</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>37.16</td>\n",
       "      <td>8604.808989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2083</th>\n",
       "      <td>84.704544</td>\n",
       "      <td>11600.263788</td>\n",
       "      <td>68.319372</td>\n",
       "      <td>5156.374866</td>\n",
       "      <td>74.126128</td>\n",
       "      <td>5909.092779</td>\n",
       "      <td>47.904010</td>\n",
       "      <td>1284.785583</td>\n",
       "      <td>75.365711</td>\n",
       "      <td>13410.053991</td>\n",
       "      <td>128.069187</td>\n",
       "      <td>17892.248830</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>27.90</td>\n",
       "      <td>10379.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2084</th>\n",
       "      <td>86.411254</td>\n",
       "      <td>15363.223977</td>\n",
       "      <td>73.552324</td>\n",
       "      <td>4861.056585</td>\n",
       "      <td>65.636763</td>\n",
       "      <td>3135.214727</td>\n",
       "      <td>46.339966</td>\n",
       "      <td>1127.833136</td>\n",
       "      <td>79.049796</td>\n",
       "      <td>13535.073084</td>\n",
       "      <td>143.509261</td>\n",
       "      <td>29796.278936</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>24.81</td>\n",
       "      <td>12371.488889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2085</th>\n",
       "      <td>80.804315</td>\n",
       "      <td>11892.618570</td>\n",
       "      <td>66.684596</td>\n",
       "      <td>4529.339198</td>\n",
       "      <td>77.156680</td>\n",
       "      <td>7107.676764</td>\n",
       "      <td>41.084834</td>\n",
       "      <td>888.953109</td>\n",
       "      <td>78.357583</td>\n",
       "      <td>14206.653984</td>\n",
       "      <td>120.761672</td>\n",
       "      <td>17445.903824</td>\n",
       "      <td>1</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>26.26</td>\n",
       "      <td>12478.788889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         hsa03040.mean  hsa03040.var  hsa03050.mean  hsa03050.var  \\\n",
       "studyID                                                             \n",
       "2001         86.154219  21800.778325      79.794512   7178.102233   \n",
       "2002         97.584318  15007.706572      68.788322   3862.390316   \n",
       "2003         95.057381  12617.681555      68.462327   3563.701033   \n",
       "2004         88.907581  18117.487593      82.024351   7074.260758   \n",
       "2006         84.981499   9910.820112      79.067493   6394.689793   \n",
       "2008         82.929690  11331.997415      66.657281   4586.447542   \n",
       "2010         85.355635  11538.139061      73.918468   7730.427403   \n",
       "2012         79.392239  11264.853351      71.502587   4623.222450   \n",
       "2013         78.975281  11137.403893      75.901554   6313.789709   \n",
       "2014         87.986462  15223.094407      66.645613   2980.177737   \n",
       "2017         77.755436  10616.639173      71.696635   4116.902398   \n",
       "2021         84.305451  19095.821110      78.528183   6753.366277   \n",
       "2022         89.930269  11210.028770      76.787824   6479.607009   \n",
       "2023         80.794456  13378.032499      71.638355   6861.057579   \n",
       "2024         88.300963  16023.856548      77.164995   5786.831748   \n",
       "2025         89.899460  13346.454263      74.136285   4309.389757   \n",
       "2026         90.063112  14508.418809      66.363304   3393.824978   \n",
       "2027         87.965760  15990.147424      78.535297   6267.562339   \n",
       "2028         93.677503  13275.656212      70.778871   3471.347787   \n",
       "2029         86.837443  12777.916452      79.732441   6472.807282   \n",
       "2031         83.304774  13847.286350      87.231774   9472.004453   \n",
       "2032         84.599143  15076.090800      77.215088   6320.330961   \n",
       "2033         91.443304  14426.500034      74.061203   4653.030840   \n",
       "2035         89.742029  13641.656921      68.352144   4025.952556   \n",
       "2037         88.493816  18977.480431      78.903030   6658.529322   \n",
       "2039         93.624940  15353.276799      73.941307   4836.814917   \n",
       "2041         80.031678  13067.512235      90.442608  10003.348912   \n",
       "2043         86.465604  12329.403094      75.059489   5550.219294   \n",
       "2044         89.553404  14247.092850      82.023975   5922.234284   \n",
       "2045         82.970524  12717.350417      74.575575   6783.391260   \n",
       "2046         99.106002  13132.237552      68.486609   3142.365681   \n",
       "2048         90.142907  12099.007956      73.615715   5697.750368   \n",
       "2050         92.261336  16844.280401      74.742986   7379.447844   \n",
       "2054         89.731348  20029.907545      65.600004   5081.444090   \n",
       "2055         96.257339  13929.679246      71.490377   3893.740275   \n",
       "2056         83.867171  14193.854171      71.015505   5074.283124   \n",
       "2057         90.695080  16151.968610      75.381286   6656.041090   \n",
       "2058         91.945818  16277.789919      69.678581   4732.540215   \n",
       "2059         88.161458  14606.067906      66.979994   3673.307013   \n",
       "2060         93.574857  15257.636477      71.204814   3909.931936   \n",
       "2063         89.245037  11176.705766      70.628577   3347.309704   \n",
       "2066         84.904408  16842.328607      71.120997   6456.650526   \n",
       "2067         85.165135  13255.925650      73.960563   6143.702706   \n",
       "2069         88.345350  12101.396712      68.961044   4033.150688   \n",
       "2070         80.465605  15969.087581      87.158230  15931.926789   \n",
       "2071         94.909795  12911.268116      64.009146   2963.523683   \n",
       "2072         92.760542  16757.182832      73.523155   4477.140378   \n",
       "2074         83.118367  10945.902014      74.709817   4991.980685   \n",
       "2075         94.319425  12512.572591      73.736107   3746.418588   \n",
       "2078         85.742390  12729.406681      70.930927   4162.966425   \n",
       "2080         86.098120  16357.847727      70.979893   7001.641242   \n",
       "2081         94.336278  19677.124463      77.345154   7144.937707   \n",
       "2082         87.532946  10669.121344      69.246463   4164.073298   \n",
       "2083         84.704544  11600.263788      68.319372   5156.374866   \n",
       "2084         86.411254  15363.223977      73.552324   4861.056585   \n",
       "2085         80.804315  11892.618570      66.684596   4529.339198   \n",
       "\n",
       "         hsa03060.mean  hsa03060.var  hsa04130.mean  hsa04130.var  \\\n",
       "studyID                                                             \n",
       "2001         72.230671   3886.474530      50.664777   1633.094649   \n",
       "2002        101.742322   5646.975137      43.465866   1679.074231   \n",
       "2003        102.477762  11260.700282      49.024537   1575.377166   \n",
       "2004         82.144727   4236.446791      48.366668   2024.659491   \n",
       "2006         96.999589   9473.002696      48.343406   1081.223172   \n",
       "2008         79.140014   4713.764474      44.445032   1542.721675   \n",
       "2010         70.030188   2029.684544      43.418717    989.941485   \n",
       "2012         85.543070   8752.621082      41.999912    697.849831   \n",
       "2013         74.507146   3365.581198      49.651971    993.962174   \n",
       "2014         94.312513   8191.870489      43.780435    872.193056   \n",
       "2017         77.247865   7836.542319      45.797804   1360.120681   \n",
       "2021         96.205240  18887.341459      45.270453   1189.128157   \n",
       "2022         91.144792   7384.220616      45.243613    937.190014   \n",
       "2023         68.517151   4977.711932      49.310246   1507.653136   \n",
       "2024         96.027550   6805.130877      50.184482   1706.580490   \n",
       "2025         87.456163   4889.118972      47.655724   1490.806989   \n",
       "2026         89.123116   8152.698244      51.435005   1723.421767   \n",
       "2027         87.762394   4773.675717      45.369075   1067.351096   \n",
       "2028        104.425612  14447.066268      45.469176   1438.612528   \n",
       "2029         79.335992   3034.885833      42.800341    813.354228   \n",
       "2031         81.831085   3198.271171      43.625473   1359.819042   \n",
       "2032         84.280116   4353.298933      50.953525   1561.893886   \n",
       "2033         97.494060   8429.803001      42.320264    740.599363   \n",
       "2035         84.795618   5588.773860      43.415356   1220.701921   \n",
       "2037         86.744563   4871.696860      47.545816   1473.574541   \n",
       "2039         89.770629   6283.943022      40.108883    864.476649   \n",
       "2041         83.743442   5018.654118      43.492050   1030.933741   \n",
       "2043         93.121017   7637.048792      43.252300   1110.147691   \n",
       "2044         87.031223   4133.966007      42.387568    781.423972   \n",
       "2045         85.570066   7133.801809      46.162720   1430.185268   \n",
       "2046        100.022804   6862.608329      40.511760    691.291473   \n",
       "2048         97.552240   8180.729473      44.508669    795.479967   \n",
       "2050         84.651212   5783.691802      48.895375   1368.203442   \n",
       "2054         70.618649   3169.815461      47.018640   1328.109248   \n",
       "2055         91.675426   5426.960873      46.023319    959.155122   \n",
       "2056         87.108616  13083.056579      53.392858   1728.275747   \n",
       "2057         84.650089   7530.567704      51.780389   1815.543035   \n",
       "2058         86.962484   7390.052685      51.739166   1464.739633   \n",
       "2059         80.762856   6687.343045      47.626025   1262.257970   \n",
       "2060         94.814563   5031.257154      46.736111   1515.774999   \n",
       "2063         95.430374   9385.516058      41.538239    469.388352   \n",
       "2066         79.107568   4211.791912      47.927609   1057.170103   \n",
       "2067         80.197836   4663.151581      51.550945   1208.524718   \n",
       "2069         90.122411   7503.729522      47.301401    973.658611   \n",
       "2070         71.814622   3428.432314      56.186365   1754.897754   \n",
       "2071        100.214998  12428.768562      46.198503   1130.240997   \n",
       "2072         88.223900   4594.908620      42.113304    806.445894   \n",
       "2074         86.465732   6730.052489      42.295604   1381.937235   \n",
       "2075        102.554280   9611.344926      42.600133   1030.573381   \n",
       "2078         75.298183   5966.647276      42.831349   1022.180817   \n",
       "2080         71.187518   3715.127292      48.427998   1643.900517   \n",
       "2081         81.063363   6756.216948      50.198273   1480.592118   \n",
       "2082        100.206618  11937.233087      46.501057   1330.750730   \n",
       "2083         74.126128   5909.092779      47.904010   1284.785583   \n",
       "2084         65.636763   3135.214727      46.339966   1127.833136   \n",
       "2085         77.156680   7107.676764      41.084834    888.953109   \n",
       "\n",
       "         hsa04141.mean  hsa04141.var  hsa04662.mean  hsa04662.var  Group  Age  \\\n",
       "studyID                                                                         \n",
       "2001         98.424988  49954.700683     159.915149  37825.957918      0   28   \n",
       "2002         84.057955  15199.893980     102.296825   9691.314802      1   38   \n",
       "2003         82.168618  11999.813861     106.311380  13742.844297      0   26   \n",
       "2004         97.786337  50042.642204     127.510289  20393.076993      1   60   \n",
       "2006         75.811154   9582.504013     129.117374  18994.681879      1   48   \n",
       "2008         92.440769  35019.500296     128.875116  21765.144484      0   46   \n",
       "2010         75.195009  12947.641050     121.467418  17934.217227      1   25   \n",
       "2012         90.236760  39247.843464     132.935727  27016.620548      1   29   \n",
       "2013         84.045117  13879.697868     140.987077  25844.145886      1   68   \n",
       "2014         89.732871  32561.387584     124.160901  18185.938066      0   47   \n",
       "2017         77.455720  16302.483979     120.939594  21636.959062      0   43   \n",
       "2021         94.273918  19673.172826     136.280128  23509.834271      0   61   \n",
       "2022         83.561518  21902.492550     115.160412  16076.004261      1   38   \n",
       "2023         70.188524   9720.854821     147.573828  28635.270637      1   26   \n",
       "2024         89.694460  21883.962547     121.631534  19238.969494      0   31   \n",
       "2025         87.966418  31171.890319     117.686425  18595.445416      1   41   \n",
       "2026         80.378923  16456.469727     128.433052  20875.689466      1   45   \n",
       "2027         84.632726  14455.992459     135.383158  22817.918896      1   27   \n",
       "2028         98.088122  44218.740718     104.052830  13351.055102      0   29   \n",
       "2029         77.095396  12954.634462     113.800537  16342.677332      0   65   \n",
       "2031         87.186804  25676.322767     135.052200  24490.830479      1   55   \n",
       "2032         85.047436  21987.069585     134.926364  22148.317176      0   37   \n",
       "2033         77.482798  10198.057425     120.576410  16771.091209      0   47   \n",
       "2035         81.904858  15534.413694     114.604942  13748.974679      0   52   \n",
       "2037         89.300040  29976.301936     134.420417  23353.288066      0   54   \n",
       "2039         77.506434   8752.024963     112.076397  15239.603971      0   30   \n",
       "2041         82.609518  19682.190262     131.169521  21764.476968      1   78   \n",
       "2043         79.365612  15510.010492     121.598649  17240.654659      1   44   \n",
       "2044         71.818587   5492.976344     126.783921  17723.167896      0   54   \n",
       "2045         97.719079  61376.614054     134.111570  25805.889655      1   41   \n",
       "2046         87.248678  18915.603559      99.686216  11418.570726      0   33   \n",
       "2048         76.494275   9859.374586     115.196916  14010.076171      1   43   \n",
       "2050         72.303895   5401.761608     144.392211  27967.615539      0   35   \n",
       "2054         85.070438  24412.719845     142.054988  27701.952087      1   29   \n",
       "2055         78.144561   7950.437283     120.653457  18534.375546      0   45   \n",
       "2056         76.659719   8163.592298     146.430768  29788.762730      0   40   \n",
       "2057         85.988297  19463.255531     141.308048  27256.647749      1   40   \n",
       "2058         79.651849  14018.276563     147.583343  29334.118119      1   36   \n",
       "2059         78.271969  12096.209910     126.417151  19818.482358      1   38   \n",
       "2060         79.785701  10052.618329     127.165161  20044.039832      1   42   \n",
       "2063         76.277565   7148.296414     116.229302  14822.815296      0   26   \n",
       "2066         87.973484  26837.327336     139.761651  28263.382365      1   50   \n",
       "2067         79.389044  11722.503280     140.346213  24172.835629      0   67   \n",
       "2069         81.623828  16475.039268     121.649818  17242.368864      0   44   \n",
       "2070         95.326720  34261.002217     164.485529  44005.384666      1   53   \n",
       "2071         75.687219   7265.876517     121.131946  15827.414438      1   49   \n",
       "2072         80.784976  11308.600052     129.751098  22728.937153      0   52   \n",
       "2074         89.948690  39501.124399     111.804078  13088.737936      0   63   \n",
       "2075         74.983478   7150.664821     109.713684  14285.898424      0   33   \n",
       "2078         75.304986   9978.306786     120.234976  16627.451104      1   58   \n",
       "2080         74.891012   9204.929276     158.004802  35318.290323      1   41   \n",
       "2081         77.077226  11062.048165     152.984761  33089.092366      1   30   \n",
       "2082         77.054105  10045.199240     112.856439  13096.204865      0   45   \n",
       "2083         75.365711  13410.053991     128.069187  17892.248830      1   52   \n",
       "2084         79.049796  13535.073084     143.509261  29796.278936      0   25   \n",
       "2085         78.357583  14206.653984     120.761672  17445.903824      1   39   \n",
       "\n",
       "         Gender    BMI         Steps  \n",
       "studyID                               \n",
       "2001          0  36.24  10723.455556  \n",
       "2002          1  35.71   4654.916667  \n",
       "2003          1  42.80   5675.133333  \n",
       "2004          0  26.72   9855.843373  \n",
       "2006          1  43.89   7595.388889  \n",
       "2008          1  29.81  13632.400000  \n",
       "2010          1  26.33   6268.666667  \n",
       "2012          0  24.84  10499.788462  \n",
       "2013          1  27.68   4357.840000  \n",
       "2014          0  28.03   9324.196429  \n",
       "2017          0  28.58  10186.133333  \n",
       "2021          1  28.90   5194.886364  \n",
       "2022          1  26.05  11843.067416  \n",
       "2023          1  30.17  12333.911111  \n",
       "2024          1  27.83  13218.820225  \n",
       "2025          0  28.37  11552.678161  \n",
       "2026          0  28.56  18288.966667  \n",
       "2027          1  29.34   5516.397590  \n",
       "2028          0  32.68   6424.600000  \n",
       "2029          0  27.19  10695.822222  \n",
       "2031          1  30.52  11053.466667  \n",
       "2032          0  32.30  13450.933333  \n",
       "2033          0  33.59   9783.255556  \n",
       "2035          1  31.41  12545.133333  \n",
       "2037          0  28.51  10382.922222  \n",
       "2039          1  28.99   9360.922222  \n",
       "2041          1  28.52   8925.269663  \n",
       "2043          1  26.63   7866.140845  \n",
       "2044          1  28.50   7828.922222  \n",
       "2045          0  26.42   7802.383721  \n",
       "2046          0  25.03   6550.046512  \n",
       "2048          1  17.71  10610.886364  \n",
       "2050          1  24.17   9727.197674  \n",
       "2054          1  27.48  10792.277778  \n",
       "2055          0  22.46   8528.325581  \n",
       "2056          1  28.29  12033.955556  \n",
       "2057          0  24.26  14191.677778  \n",
       "2058          1  37.55   3087.068182  \n",
       "2059          0  30.08  10746.465909  \n",
       "2060          0  23.55   7947.636364  \n",
       "2063          0  37.58   7116.400000  \n",
       "2066          1  27.75  11816.044444  \n",
       "2067          1  28.70   3800.273973  \n",
       "2069          0  28.70  13101.033333  \n",
       "2070          1  35.36  10405.622222  \n",
       "2071          0  37.34   9117.604651  \n",
       "2072          0  27.46   8744.563218  \n",
       "2074          1  21.63   7098.400000  \n",
       "2075          0  31.62   9615.105882  \n",
       "2078          1  24.26  11135.222222  \n",
       "2080          1  30.76   7823.833333  \n",
       "2081          0  31.08  11297.533333  \n",
       "2082          1  37.16   8604.808989  \n",
       "2083          0  27.90  10379.583333  \n",
       "2084          1  24.81  12371.488889  \n",
       "2085          1  26.26  12478.788889  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2.,  5.,  5.,  6.,  7., 10.,  5.,  6.,  6.,  4.]),\n",
       " array([ 65.63676281,  69.51564775,  73.39453269,  77.27341763,\n",
       "         81.15230257,  85.03118751,  88.91007245,  92.78895739,\n",
       "         96.66784233, 100.54672727, 104.42561221]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X['hsa03060.mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random_state = [100, 122, 200, 300, 368, 400, 500, 600, 700, 22]\n",
    "no_band_train_acc = []\n",
    "no_band_test_acc = []\n",
    "\n",
    "for i in random_state :\n",
    "    \n",
    "    train_df, test_df = train_test_split(no_band_final, test_size=0.2, random_state= i)\n",
    "    \n",
    "    X_train = train_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_train = train_df['Weight_loss_band']  \n",
    "    \n",
    "    X_test  = test_df.drop('Weight_loss_band', axis=1)\n",
    "    Y_test = test_df['Weight_loss_band']  \n",
    "    \n",
    "    \n",
    "    clf = svm.SVC(kernel = 'linear')\n",
    "    clf.fit(X_train, Y_train)  \n",
    "\n",
    "    svm_train_acc = clf.score(X_train, Y_train)\n",
    "    svm_test_acc = clf.score(X_test, Y_test)\n",
    "    \n",
    "    no_band_train_acc.append(svm_train_acc)\n",
    "    no_band_test_acc.append(svm_test_acc)\n",
    "    \n",
    "    print('Random State: ', i)\n",
    "    \n",
    "    print('Training Accuracy:', svm_train_acc)\n",
    "    \n",
    "    print('Test Accuracy:', svm_test_acc)\n",
    "\n",
    "print('Mean Training Accuracy:', mean(no_band_train_acc)) \n",
    "print('Mean Test Accuracy:', mean(no_band_test_acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n",
      "Features used: 17\n",
      "SVM: The training accuracy is: 1.0\n",
      "SVM: The testing accuracy is: 0.7058823529411765\n"
     ]
    }
   ],
   "source": [
    "# Recursive Feature Elimination\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "\n",
    "# create a base classifier used to evaluate a subset of attributes\n",
    "svm = SVC(C = 150, kernel = 'rbf', max_iter = 10000, gamma = 'scale')\n",
    "\n",
    "svm_features_included = []\n",
    "svm_scores = []\n",
    "\n",
    "n = 1\n",
    "\n",
    "## for i in range(len(columns)):\n",
    "for i in range(len(X.columns)) :\n",
    "\n",
    "# keep the top n features\n",
    "\n",
    "    #svm_rfe = RFE(svm, n)\n",
    "    #svm_rfe = svm_rfe.fit(X, y)\n",
    "\n",
    "##update X dataframe to include only selected features\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X.iloc[:, svm_rfe.support_], y, test_size=0.3, random_state=32)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=32)\n",
    "\n",
    "\n",
    "## run regression\n",
    "    svm.fit(X_train, y_train)\n",
    "\n",
    "    svm_train_acc = svm.score(X_train, y_train)\n",
    "    svm_test_acc = svm.score(X_test, y_test)\n",
    "   \n",
    "    svm_features_included.append(X.iloc[:, svm_rfe.support_].columns.values)\n",
    "    svm_scores.append(svm_test_acc)\n",
    "\n",
    "    print('Features used:', len(X_train.columns.values))\n",
    "    \n",
    "    print('SVM: The training accuracy is:', svm_train_acc)\n",
    "    print('SVM: The testing accuracy is:', svm_test_acc)\n",
    "\n",
    "\n",
    "\n",
    "##update n\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost labeling accuracy: 70.59 %\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# XGBoost, same API as scikit-learn\n",
    "gradboost = xgb.XGBClassifier(max_depth = 10, gamma = 6, n_estimators=1000)             # instantiate\n",
    "gradboost.fit(X_train, y_train)                              # fit\n",
    "acc_xgboost = gradboost.score(X_test, y_test)                  # predict + evalute\n",
    "\n",
    "print('XGBoost labeling accuracy:', str(round(acc_xgboost*100,2)),'%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
